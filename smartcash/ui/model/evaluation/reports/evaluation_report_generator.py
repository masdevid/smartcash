"""
Evaluation Report Generator (Optimized)
Dedicated report generator for evaluation module.
"""

from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path
import json


class EvaluationReportGenerator:
    """Optimized report generator for evaluation module."""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.templates = {
            'performance': '<div style="font-size:0.9em;margin-bottom:15px;"><h4 style="margin-bottom:10px;color:#2c3e50;">üìä Performance</h4><div><strong>Best Model:</strong> {best_model}</div><div><strong>Average mAP:</strong> {avg_map:.3f}</div><div><strong>Grade:</strong> {grade}</div><div><strong>Models:</strong> {models_evaluated}</div><div><strong>Recommendation:</strong> {recommendation}</div></div>',
            'comparison': '<div style="font-size:0.9em;margin-bottom:15px;"><h4 style="margin-bottom:10px;color:#2c3e50;">üîç Model Comparison</h4><div><strong>Models:</strong> {models_evaluated}</div><div><strong>Winner:</strong> {best_model}</div><div><strong>Gap:</strong> {performance_gap}</div><div><strong>Runtime:</strong> {runtime_info}</div><div><strong>Next Steps:</strong> {next_steps}</div></div>',
            'scenarios': '<div style="font-size:0.9em;margin-bottom:15px;"><h4 style="margin-bottom:10px;color:#2c3e50;">üéØ Scenarios</h4>{scenario_details}</div>',
            'metrics': '<div style="font-size:0.9em;margin-bottom:15px;"><h4 style="margin-bottom:10px;color:#2c3e50;">üìà Metrics</h4>{metrics_table}</div>',
            'summary': '<div style="background:#f8f9fa;padding:15px;border-left:4px solid #007bff;margin-bottom:20px;"><h3 style="margin-top:0;color:#007bff;">üìã Summary</h3><p><strong>Date:</strong> {timestamp}</p><p><strong>Duration:</strong> {duration}</p><p><strong>Key Finding:</strong> {key_finding}</p><p><strong>Action:</strong> {action_required}</p></div>'
        }
    
    def generate_comprehensive_report(self, results: Dict[str, Any]) -> str:
        """Generate comprehensive evaluation report."""
        try:
            data = self._extract_data(results)
            sections = [
                self._generate_summary(data),
                self._generate_performance_report(data),
                self._generate_model_comparison_report(data),
                self._generate_scenario_breakdown(data),
                self._generate_detailed_metrics(data)
            ]
            footer = f'<div style="margin-top:20px;font-size:0.8em;color:#666;text-align:center;">Generated by SmartCash at {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</div>'
            return f'<div style="font-family:\'Segoe UI\',sans-serif;">{"".join(sections)}{footer}</div>'
        except Exception as e:
            return f'<div style="color:red;">‚ùå Report generation failed: {e}</div>'
    
    def _extract_data(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Extract and normalize data for report generation."""
        data = {
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'best_model': self._extract_best_model(results),
            'avg_map': self._calculate_average_map(results),
            'models_evaluated': self._count_models(results),
            'scenarios_completed': self._get_scenarios(results),
            'duration': results.get('duration', 'N/A'),
            'evaluation_results': results.get('evaluation_results', {}),
            'raw_results': results
        }
        
        data.update({
            'grade': self._calculate_grade(data['avg_map']),
            'recommendation': self._get_recommendation(data['avg_map']),
            'performance_gap': self._calculate_gap(results),
            'key_finding': self._extract_finding(data),
            'action_required': self._determine_action(data)
        })
        return data
    
    def _generate_summary(self, data: Dict[str, Any]) -> str:
        return self.templates['summary'].format(**data)
    
    def _generate_performance_report(self, data: Dict[str, Any]) -> str:
        return self.templates['performance'].format(**data)
    
    def _generate_model_comparison_report(self, data: Dict[str, Any]) -> str:
        data.update({
            'runtime_info': self._get_runtime_info(data),
            'next_steps': self._get_next_steps(data)
        })
        return self.templates['comparison'].format(**data)
    
    def _generate_scenario_breakdown(self, data: Dict[str, Any]) -> str:
        scenarios = data.get('scenarios_completed', [])
        if isinstance(scenarios, int):
            scenarios = ['position_variation', 'lighting_variation'][:scenarios]
        
        details = ''.join([f'<div style="margin-bottom:8px;padding:8px;background:#f8f9fa;border-radius:4px;"><strong>‚Ä¢ {scenario.replace("_", " ").title()}:</strong> ‚úÖ Completed<div style="margin-left:15px;font-size:0.85em;color:#666;">{self._get_scenario_results(data, scenario)}</div></div>' for scenario in scenarios])
        return self.templates['scenarios'].format(scenario_details=details or '<div>No scenarios completed</div>')
    
    def _generate_detailed_metrics(self, data: Dict[str, Any]) -> str:
        evaluation_results = data.get('evaluation_results', {})
        if not evaluation_results:
            return self.templates['metrics'].format(metrics_table='<div>No metrics available</div>')
        
        rows = []
        for scenario, scenario_results in evaluation_results.items():
            for backbone, backbone_data in scenario_results.items():
                metrics = backbone_data.get('metrics', {})
                checkpoint_info = backbone_data.get('checkpoint_info', {})
                rows.append(f'<tr><td style="padding:8px;border:1px solid #ddd;">{scenario.replace("_", " ").title()}</td><td style="padding:8px;border:1px solid #ddd;">{backbone}</td><td style="padding:8px;border:1px solid #ddd;">{metrics.get("mAP", "N/A")}</td><td style="padding:8px;border:1px solid #ddd;">{metrics.get("precision", "N/A")}</td><td style="padding:8px;border:1px solid #ddd;">{metrics.get("recall", "N/A")}</td><td style="padding:8px;border:1px solid #ddd;">{checkpoint_info.get("display_name", "N/A")}</td></tr>')
        
        table = f'<table style="width:100%;border-collapse:collapse;font-size:0.85em;"><thead><tr style="background:#f8f9fa;"><th style="padding:10px;border:1px solid #ddd;text-align:left;">Scenario</th><th style="padding:10px;border:1px solid #ddd;text-align:left;">Backbone</th><th style="padding:10px;border:1px solid #ddd;text-align:left;">mAP</th><th style="padding:10px;border:1px solid #ddd;text-align:left;">Precision</th><th style="padding:10px;border:1px solid #ddd;text-align:left;">Recall</th><th style="padding:10px;border:1px solid #ddd;text-align:left;">Model</th></tr></thead><tbody>{"".join(rows)}</tbody></table>'
        return self.templates['metrics'].format(metrics_table=table)
    
    def generate_json_report(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate structured JSON report."""
        data = self._extract_data(results)
        return {
            'metadata': {'generated_at': data['timestamp'], 'generator_version': '1.0.0', 'report_type': 'evaluation_comprehensive'},
            'summary': {k: data[k] for k in ['best_model', 'avg_map', 'models_evaluated', 'scenarios_completed', 'grade', 'recommendation']},
            'detailed_results': data.get('evaluation_results', {}),
            'metrics_analysis': self._generate_metrics_analysis(data),
            'recommendations': self._generate_recommendations(data)
        }
    
    def export_report(self, results: Dict[str, Any], output_path: str, format: str = 'html') -> bool:
        """Export evaluation report to file."""
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            if format.lower() == 'html':
                content = f'<!DOCTYPE html><html><head><title>SmartCash Evaluation Report</title><meta charset="utf-8"><style>body{{font-family:\'Segoe UI\',sans-serif;margin:20px;}}.evaluation-report{{max-width:1200px;margin:0 auto;}}</style></head><body>{self.generate_comprehensive_report(results)}</body></html>'
            elif format.lower() == 'json':
                content = json.dumps(self.generate_json_report(results), indent=2)
            elif format.lower() == 'txt':
                content = self._generate_text_report(results)
            else:
                raise ValueError(f"Unsupported format: {format}")
            
            output_path.write_text(content, encoding='utf-8')
            return True
        except Exception:
            return False
    
    # Additional methods for evaluation module integration
    
    def generate_evaluation_report(self, results: Dict[str, Any], timestamp: str = None) -> str:
        if timestamp: results['timestamp'] = timestamp
        return self.generate_comprehensive_report(results)
    
    def generate_scan_report(self, models_available: int, evaluation_enabled: bool) -> str:
        status_icon = "‚úÖ" if evaluation_enabled else "‚ö†Ô∏è"
        status_text = "Ready for evaluation" if evaluation_enabled else "Evaluation not enabled"
        return f'<div style="font-family:\'Segoe UI\',sans-serif;padding:15px;background:#f8f9fa;border-radius:8px;"><h4 style="margin-top:0;color:#2c3e50;">üîç Model Scan Results</h4><div><strong>Models Available:</strong> {models_available}</div><div><strong>Status:</strong> {status_icon} {status_text}</div><div><strong>Scan Completed:</strong> {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</div>{self._get_scan_recommendations(models_available, evaluation_enabled)}</div>'
    
    def generate_empty_state(self) -> str:
        return '<div style="font-family:\'Segoe UI\',sans-serif;padding:20px;text-align:center;color:#666;"><h4 style="color:#888;">üìä No Evaluation Results</h4><p>Run an evaluation to see detailed performance analysis and recommendations.</p><div style="margin-top:15px;font-size:0.9em;"><div>‚Ä¢ Click "Run Evaluation" to start comprehensive model testing</div><div>‚Ä¢ Results will include performance metrics and recommendations</div><div>‚Ä¢ Reports can be exported in multiple formats</div></div></div>'
    
    # Helper methods (optimized)
    
    def _extract_best_model(self, results: Dict[str, Any]) -> str:
        if 'best_model' in results: return str(results['best_model'])
        if 'summary' in results and 'best_configurations' in results['summary']:
            best_configs = results['summary']['best_configurations']
            if best_configs and 'model_name' in best_configs: return str(best_configs['model_name'])
        
        if 'evaluation_results' in results:
            best_map, best_model = 0, 'N/A'
            for scenario_results in results['evaluation_results'].values():
                for backbone, backbone_results in scenario_results.items():
                    current_map = backbone_results.get('metrics', {}).get('mAP', 0)
                    if current_map > best_map:
                        best_map = current_map
                        best_model = backbone_results.get('checkpoint_info', {}).get('display_name', backbone)
            return best_model
        return 'N/A'
    
    def _calculate_average_map(self, results: Dict[str, Any]) -> float:
        if 'metrics' in results and 'mAP' in results['metrics']: return float(results['metrics']['mAP'])
        if 'summary' in results and 'aggregated_metrics' in results['summary']:
            if 'average_map' in results['summary']['aggregated_metrics']: return float(results['summary']['aggregated_metrics']['average_map'])
        
        if 'evaluation_results' in results:
            map_scores = [metrics['mAP'] for scenario_results in results['evaluation_results'].values() for backbone_results in scenario_results.values() for metrics in [backbone_results.get('metrics', {})] if 'mAP' in metrics]
            return sum(map_scores) / len(map_scores) if map_scores else 0.0
        return 0.0
    
    def _count_models(self, results: Dict[str, Any]) -> int:
        if 'models_evaluated' in results: return int(results['models_evaluated'])
        if 'evaluation_results' in results:
            return len(set(backbone for scenario_results in results['evaluation_results'].values() for backbone in scenario_results.keys()))
        return 0
    
    def _get_scenarios(self, results: Dict[str, Any]) -> List[str]:
        if 'scenarios_completed' in results:
            scenarios = results['scenarios_completed']
            if isinstance(scenarios, list): return scenarios
            if isinstance(scenarios, int): return ['position_variation', 'lighting_variation'][:scenarios]
        if 'evaluation_results' in results: return list(results['evaluation_results'].keys())
        return []
    
    def _calculate_grade(self, avg_map: float) -> str:
        if avg_map >= 0.9: return "üèÜ Excellent (A+)"
        elif avg_map >= 0.8: return "‚≠ê Very Good (A)"
        elif avg_map >= 0.7: return "‚úÖ Good (B)"
        elif avg_map >= 0.6: return "‚ö†Ô∏è Fair (C)"
        else: return "‚ùå Needs Improvement (D)"
    
    def _get_recommendation(self, avg_map: float) -> str:
        if avg_map >= 0.85: return "Model ready for production deployment"
        elif avg_map >= 0.75: return "Consider additional training or data augmentation"
        elif avg_map >= 0.65: return "Review training parameters and data quality"
        else: return "Significant improvements needed - check model architecture"
    
    def _calculate_gap(self, results: Dict[str, Any]) -> str:
        try:
            if 'evaluation_results' not in results: return "Analysis in progress"
            map_scores = [metrics.get('mAP', 0) for scenario_results in results['evaluation_results'].values() for backbone_results in scenario_results.values() for metrics in [backbone_results.get('metrics', {})] if 'mAP' in metrics]
            return f"{max(map_scores) - min(map_scores):.3f} mAP difference" if len(map_scores) >= 2 else "Insufficient data"
        except: return "Gap analysis failed"
    
    def _extract_finding(self, data: Dict[str, Any]) -> str:
        best_model, avg_map = data.get('best_model', 'N/A'), data.get('avg_map', 0.0)
        if avg_map >= 0.8: return f"{best_model} achieved excellent performance (mAP: {avg_map:.3f})"
        elif avg_map >= 0.65: return f"{best_model} shows good potential with room for improvement"
        else: return "Models require significant optimization for production use"
    
    def _determine_action(self, data: Dict[str, Any]) -> str:
        avg_map = data.get('avg_map', 0.0)
        if avg_map >= 0.85: return "Ready for deployment - conduct final validation"
        elif avg_map >= 0.7: return "Consider optimization before deployment"
        else: return "Requires model improvement before deployment"
    
    def _get_runtime_info(self, data: Dict[str, Any]) -> str:
        duration, models = data.get('duration', 'N/A'), data.get('models_evaluated', 0)
        return f"Evaluated {models} models in {duration}" if duration != 'N/A' and models > 0 else "Runtime analysis not available"
    
    def _get_next_steps(self, data: Dict[str, Any]) -> str:
        avg_map = data.get('avg_map', 0.0)
        if avg_map >= 0.8: return "Deploy best model or conduct additional validation testing"
        elif avg_map >= 0.65: return "Fine-tune hyperparameters or increase training data"
        else: return "Consider different model architecture or review data quality"
    
    def _get_scenario_results(self, data: Dict[str, Any], scenario: str) -> str:
        try:
            scenario_data = data.get('evaluation_results', {}).get(scenario, {})
            if not scenario_data: return "No detailed results available"
            
            best_map, best_backbone = 0, 'N/A'
            for backbone, backbone_data in scenario_data.items():
                current_map = backbone_data.get('metrics', {}).get('mAP', 0)
                if current_map > best_map:
                    best_map, best_backbone = current_map, backbone
            return f"Best: {best_backbone} (mAP: {best_map:.3f})"
        except: return "Analysis failed"
    
    def _get_scan_recommendations(self, models_available: int, evaluation_enabled: bool) -> str:
        if models_available == 0: return '<div style="margin-top:15px;padding:10px;background:#fff3cd;border-left:4px solid #ffc107;border-radius:4px;"><strong>‚ö†Ô∏è No Models Found</strong><br>Build or download models before running evaluation.</div>'
        elif not evaluation_enabled: return f'<div style="margin-top:15px;padding:10px;background:#d4edda;border-left:4px solid #28a745;border-radius:4px;"><strong>‚úÖ Models Available</strong><br>Ready to run evaluation on {models_available} model(s).</div>'
        else: return f'<div style="margin-top:15px;padding:10px;background:#d1ecf1;border-left:4px solid #17a2b8;border-radius:4px;"><strong>üöÄ Ready for Evaluation</strong><br>{models_available} model(s) available for comprehensive testing.</div>'
    
    def _generate_metrics_analysis(self, data: Dict[str, Any]) -> Dict[str, Any]:
        try:
            evaluation_results = data.get('evaluation_results', {})
            analysis = {'scenario_performance': {}, 'backbone_comparison': {}, 'overall_statistics': {'mean_map': data.get('avg_map', 0.0), 'best_map': 0.0, 'worst_map': 1.0, 'variance': 0.0}}
            
            all_maps = []
            for scenario, scenario_results in evaluation_results.items():
                scenario_maps = [backbone_data.get('metrics', {}).get('mAP', 0) for backbone_data in scenario_results.values()]
                all_maps.extend(scenario_maps)
                if scenario_maps: analysis['scenario_performance'][scenario] = {'average_map': sum(scenario_maps) / len(scenario_maps), 'best_map': max(scenario_maps), 'worst_map': min(scenario_maps), 'models_tested': len(scenario_maps)}
            
            if all_maps:
                analysis['overall_statistics'].update({'best_map': max(all_maps), 'worst_map': min(all_maps)})
                if len(all_maps) > 1:
                    mean_map = sum(all_maps) / len(all_maps)
                    analysis['overall_statistics']['variance'] = sum((x - mean_map) ** 2 for x in all_maps) / len(all_maps)
            return analysis
        except: return {'error': 'Metrics analysis generation failed'}
    
    def _generate_recommendations(self, data: Dict[str, Any]) -> List[Dict[str, str]]:
        recommendations, avg_map, best_model = [], data.get('avg_map', 0.0), data.get('best_model', 'N/A')
        
        if avg_map >= 0.85: recommendations.append({'category': 'deployment', 'priority': 'high', 'action': 'Proceed with production deployment', 'rationale': f'Excellent performance achieved (mAP: {avg_map:.3f})'})
        elif avg_map >= 0.7: recommendations.append({'category': 'optimization', 'priority': 'medium', 'action': 'Consider hyperparameter tuning', 'rationale': f'Good performance with potential for improvement (mAP: {avg_map:.3f})'})
        else: recommendations.append({'category': 'architecture', 'priority': 'high', 'action': 'Review model architecture and training strategy', 'rationale': f'Performance below target threshold (mAP: {avg_map:.3f})'})
        
        if best_model != 'N/A': recommendations.append({'category': 'model_selection', 'priority': 'medium', 'action': f'Focus development on {best_model} architecture', 'rationale': 'This model showed the best performance in evaluation'})
        return recommendations
    
    def _generate_text_report(self, results: Dict[str, Any]) -> str:
        data = self._extract_data(results)
        scenarios_text = '\n'.join([f"‚Ä¢ {s.replace('_', ' ').title()}: {self._get_scenario_results(data, s)}" for s in data.get('scenarios_completed', [])])
        return f"""SMARTCASH EVALUATION REPORT
============================

Generated: {data['timestamp']}
Duration: {data['duration']}

EXECUTIVE SUMMARY
-----------------
Best Model: {data['best_model']}
Average mAP: {data['avg_map']:.3f}
Performance Grade: {data['grade']}
Models Evaluated: {data['models_evaluated']}

RECOMMENDATION
--------------
{data['recommendation']}

SCENARIO BREAKDOWN
------------------
{scenarios_text}

KEY FINDINGS
------------
{data['key_finding']}

ACTION REQUIRED
---------------
{data['action_required']}

----
Report generated by SmartCash Evaluation System"""