# Cell 7.1: Inisialisasi dan Import Data Handling - Setup awal untuk pengolahan dan manajemen dataset

# ===== 1. IMPORT DAN SETUP AWAL =====
import ipywidgets as widgets
from IPython.display import display, HTML, clear_output
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import gc
import sys
import yaml
from pathlib import Path
import torch
import pickle
from contextlib import contextmanager
from typing import Dict, Any, Optional

# Tambahkan path ke PYTHONPATH jika belum ada
if os.getcwd() not in sys.path:
    sys.path.append(os.getcwd())

# Import modul yang diperlukan
from smartcash.utils.logger import get_logger, SmartCashLogger
from smartcash.handlers.data_manager import DataManager
from smartcash.handlers.multilayer_dataset_handler import MultilayerDataManager
from smartcash.utils.enhanced_cache import EnhancedCache
from smartcash.utils.optimized_augmentation import OptimizedAugmentation

# ===== 2. UTILITY FUNCTIONS =====
@contextmanager
def memory_manager():
    """Context manager untuk mengoptimalkan penggunaan memori."""
    try:
        yield
    finally:
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def safe_load_config(config_path: str = 'configs/base_config.yaml') -> Dict[str, Any]:
    """Load config dengan penanganan error."""
    try:
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.error(f"‚ùå Error loading config: {str(e)}")
        # Fallback ke struktur config minimal jika file tidak ditemukan
        return {
            'model': {'batch_size': 16, 'workers': 2, 'img_size': [640, 640]},
            'data': {'local': {'train': 'data/train', 'valid': 'data/valid', 'test': 'data/test'}},
            'layers': ['banknote']
        }

# ===== 3. MAIN INITIALIZATION =====
# Set logger yang kompatibel dengan Colab
logger = get_logger("data_pipeline", log_to_colab=True, log_to_file=True)

with memory_manager():
    # Load config
    config = safe_load_config()
    
    # Initialize data manager dan augmentation handler
    data_manager = MultilayerDataManager(
        config=config,
        logger=logger
    )
    
    # Initialize augmentation handler
    aug_manager = OptimizedAugmentation(
        config=config,
        output_dir=None,  # Gunakan direktori default
        logger=logger,
        num_workers=min(2, config.get('model', {}).get('workers', 4))  # Batasi untuk Colab
    )
    
    # Get dataset information
    active_layers = config.get('layers', ['banknote'])
    logger.info(f"üîç Menggunakan {len(active_layers)} layer aktif: {', '.join(active_layers)}")
    
    # Get dataset statistics
    try:
        stats = data_manager.get_dataset_stats()
        logger.info("\nüìä Statistik Dataset:")
        for split, split_stats in stats.items():
            logger.info(f"{split.capitalize()}:")
            for key, value in split_stats.items():
                if isinstance(value, dict):
                    logger.info(f"  {key}:")
                    for subkey, subvalue in value.items():
                        logger.info(f"    {subkey}: {subvalue}")
                else:
                    logger.info(f"  {key}: {value}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Gagal mendapatkan statistik dataset: {str(e)}")

# ===== 4. DATA LOADERS SETUP =====
def create_dataloaders(batch_size=None, num_workers=None) -> Dict[str, Any]:
    """Create optimized data loaders with error handling."""
    batch_size = batch_size or config.get('model', {}).get('batch_size', 16)
    num_workers = num_workers or min(2, config.get('model', {}).get('workers', 2))  # Batasi untuk Colab
    
    dataloaders = {}
    try:
        dataloaders['train'] = data_manager.get_dataloader(
            split='train',
            batch_size=batch_size,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()  # Optimize GPU transfer
        )

        dataloaders['val'] = data_manager.get_dataloader(
            split='valid',
            batch_size=batch_size,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )

        dataloaders['test'] = data_manager.get_dataloader(
            split='test',
            batch_size=batch_size,
            num_workers=1,  # Reduce workers for testing
            pin_memory=torch.cuda.is_available()
        )
        
        # Log information
        logger.info(f"‚úÖ Dataloader berhasil dibuat:")
        for name, loader in dataloaders.items():
            if loader and hasattr(loader.dataset, '__len__'):
                logger.info(f"   ‚Ä¢ {name.capitalize()}: {len(loader.dataset)} gambar, {len(loader)} batch")
            else:
                logger.warning(f"   ‚Ä¢ {name.capitalize()}: loader tidak valid atau kosong")
                
        return dataloaders
    except Exception as e:
        logger.error(f"‚ùå Error creating dataloaders: {str(e)}")
        return {}

# Simpan objek penting di global untuk diakses dari cell lain
def save_globals():
    globals_dict = {
        'data_manager': data_manager,
        'aug_manager': aug_manager,
        'config': config,
        'logger': logger
    }
    try:
        with open('data_globals.pkl', 'wb') as f:
            pickle.dump(globals_dict, f)
        logger.info("üíæ Objek global disimpan untuk digunakan di cell lain")
    except Exception as e:
        logger.error(f"‚ùå Error saving globals: {str(e)}")

# Create and save dataloaders (optional)
with memory_manager():
    try:
        dataloaders = create_dataloaders()
        # Simpan objek global
        save_globals()
        logger.success("‚ú® Inisialisasi data handling selesai!")
    except Exception as e:
        logger.error(f"‚ùå Error in data initialization: {str(e)}")