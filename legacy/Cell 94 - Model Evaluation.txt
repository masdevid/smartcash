# Cell 9.4: Model Evaluation - Evaluasi model setelah training

import os
import torch
import gc
import numpy as np
import ipywidgets as widgets
from IPython.display import display, clear_output, HTML
import pickle
from pathlib import Path
from contextlib import contextmanager

# Pastikan path diatur dengan benar
import sys
if os.getcwd() not in sys.path:
    sys.path.append(os.getcwd())

# Context manager untuk manajemen memori
@contextmanager
def memory_manager():
    """Context manager untuk optimasi penggunaan memori."""
    try:
        yield
    finally:
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

# ===== 1. LOAD COMPONENTS =====
# Pastikan komponen evaluasi tersedia
try:
    # Coba muat dari training_components
    with open('training_components.pkl', 'rb') as f:
        components = pickle.load(f)
    
    pipeline = components.get('pipeline')
    model_handler = components.get('model_handler')
    checkpoint_handler = components.get('checkpoint_handler')
    experiment_tracker = components.get('experiment_tracker')
    dataloaders = components.get('dataloaders')
    
    with open('config.pkl', 'rb') as f:
        config = pickle.load(f)
    
    # Import modul-modul evaluasi
    from smartcash.utils.logger import get_logger
    from smartcash.handlers.evaluation_handler import EvaluationHandler
    
    # Import visualizer untuk visualisasi metrics
    try:
        from smartcash.utils.model_visualizer import ModelVisualizer
        visualizer = ModelVisualizer()
        visualizer_available = True
    except ImportError:
        try:
            from smartcash.utils.visualization import ResultVisualizer
            visualizer = ResultVisualizer()
            visualizer_available = True
        except ImportError:
            visualizer_available = False
    
    logger = get_logger("model_evaluation", log_to_console=True, log_to_file=True, log_to_colab=True)
    logger.info("‚úÖ Modul evaluasi berhasil dimuat")
    
    if visualizer_available:
        logger.info("üìä Visualizer tersedia untuk visualisasi metrik")
    else:
        logger.warning("‚ö†Ô∏è Visualizer tidak tersedia, menggunakan tampilan sederhana")
except Exception as e:
    print(f"‚ö†Ô∏è Komponen evaluasi belum diinisialisasi: {str(e)}")
    print("‚ÑπÔ∏è Jalankan Cell 9.1 terlebih dahulu untuk menginisialisasi pipeline")
    components = None
    pipeline = None
    model_handler = None
    checkpoint_handler = None
    experiment_tracker = None
    dataloaders = None
    config = {}
    logger = None
    visualizer_available = False

# ===== 2. UI COMPONENTS =====
# Kontrol evaluasi
model_dropdown = widgets.Dropdown(
    options=[],
    description='Model:',
    style={'description_width': 'initial'},
    layout=widgets.Layout(width='70%')
)

dataset_dropdown = widgets.Dropdown(
    options=[
        ('Dataset Test Default', None),
        ('Dataset Test (Posisi Bervariasi)', 'data/test_position_varied'),
        ('Dataset Test (Pencahayaan Bervariasi)', 'data/test_lighting_varied')
    ],
    value=None,
    description='Dataset Testing:',
    style={'description_width': 'initial'},
    layout=widgets.Layout(width='70%')
)

evaluation_settings = widgets.Accordion(children=[widgets.VBox([
    widgets.FloatSlider(
        value=0.25, 
        min=0.1, 
        max=0.9, 
        step=0.05, 
        description='Confidence Threshold:',
        style={'description_width': 'initial'}
    ),
    widgets.IntSlider(
        value=3,
        min=1,
        max=5,
        step=1,
        description='Jumlah Run:',
        style={'description_width': 'initial'}
    ),
    widgets.Checkbox(
        value=True,
        description='Tampilkan confusion matrix',
        style={'description_width': 'initial'}
    ),
    widgets.Checkbox(
        value=True,
        description='Tampilkan metrik per kelas',
        style={'description_width': 'initial'}
    )
])], titles=('Pengaturan Evaluasi',))

run_evaluation_button = widgets.Button(
    description='Evaluasi Model',
    button_style='primary',
    icon='check'
)

# Output untuk hasil evaluasi
evaluation_output = widgets.Output()

# ===== 3. INITIALIZE UI =====
def init_ui():
    """Initialize UI with available models."""
    # Load available checkpoints
    if checkpoint_handler:
        checkpoints = checkpoint_handler.list_checkpoints()
        
        options = []
        
        # Add best checkpoints if available
        if checkpoints.get('best'):
            for ckpt in checkpoints.get('best', []):
                options.append((f"Best: {ckpt.name}", str(ckpt)))
        
        # Add latest checkpoints if available
        if checkpoints.get('latest'):
            for ckpt in checkpoints.get('latest', []):
                options.append((f"Latest: {ckpt.name}", str(ckpt)))
        
        # Add epoch checkpoints if available (limit to 5)
        if checkpoints.get('epoch'):
            for ckpt in checkpoints.get('epoch', [])[:5]:
                options.append((f"Epoch: {ckpt.name}", str(ckpt)))
        
        if options:
            model_dropdown.options = options
            # Select first option as default
            model_dropdown.value = options[0][1] if options else None
        else:
            print("‚ö†Ô∏è Tidak ada checkpoint tersedia untuk evaluasi")

def on_run_evaluation_button_clicked(b):
    """Handler untuk tombol evaluasi model."""
    with evaluation_output:
        clear_output()
        
        model_path = model_dropdown.value
        test_dataset = dataset_dropdown.value
        
        if not model_path:
            print("‚ùå Pilih model checkpoint terlebih dahulu")
            return
        
        # Get evaluation settings
        conf_threshold = evaluation_settings.children[0].children[0].value
        num_runs = evaluation_settings.children[0].children[1].value
        show_confusion_matrix = evaluation_settings.children[0].children[2].value
        show_class_metrics = evaluation_settings.children[0].children[3].value
        
        # Set loading state
        run_evaluation_button.disabled = True
        run_evaluation_button.description = "Mengevaluasi..."
        
        try:
            print(f"üîç Mengevaluasi model: {model_path}")
            print(f"üìä Dataset: {test_dataset or 'Default'}")
            print(f"‚öôÔ∏è Confidence threshold: {conf_threshold}, Jumlah run: {num_runs}")
            
            # Jalankan evaluasi
            if num_runs > 1:
                results = evaluate_multiple_runs(
                    model_path, 
                    test_dataset,
                    num_runs=num_runs, 
                    conf_threshold=conf_threshold
                )
            else:
                results = evaluate_model(
                    model_path, 
                    test_dataset,
                    conf_threshold=conf_threshold
                )
            
            if results:
                print("‚úÖ Evaluasi berhasil")
                visualize_evaluation_results(
                    results, 
                    show_confusion_matrix=show_confusion_matrix,
                    show_class_metrics=show_class_metrics
                )
            else:
                print("‚ùå Evaluasi gagal atau tidak menghasilkan data")
        except Exception as e:
            print(f"‚ùå Error saat evaluasi: {str(e)}")
            import traceback
            traceback.print_exc()
        finally:
            # Reset button state
            run_evaluation_button.disabled = False
            run_evaluation_button.description = "Evaluasi Model"

# Register event handler
run_evaluation_button.on_click(on_run_evaluation_button_clicked)

# Initialize UI
init_ui()

# ===== 4. EVALUATION FUNCTIONS =====
def evaluate_model(model_path=None, test_dataset=None, conf_threshold=0.25):
    """
    Evaluasi model dengan checkpoint yang dipilih.
    
    Args:
        model_path: Path ke checkpoint model (jika None, gunakan checkpoint terbaik)
        test_dataset: Path ke dataset testing (jika None, gunakan dataset default)
        conf_threshold: Threshold confidence untuk deteksi
    
    Returns:
        Dict hasil evaluasi
    """
    try:
        with memory_manager():
            # Inisialisasi evaluator
            evaluation_handler = EvaluationHandler(
                config=config,
                logger=logger
            )
            
            # Tentukan path model jika tidak diberikan
            if model_path is None:
                if checkpoint_handler:
                    model_path = checkpoint_handler.find_best_checkpoint()
                    if not model_path:
                        logger.warning("‚ö†Ô∏è Tidak menemukan checkpoint terbaik, mencari yang terakhir...")
                        model_path = checkpoint_handler.find_latest_checkpoint()
                        
                    if not model_path:
                        logger.error("‚ùå Tidak menemukan checkpoint model")
                        return None
                else:
                    logger.error("‚ùå Checkpoint handler tidak tersedia")
                    return None
            
            # Tentukan dataset testing
            if test_dataset is None:
                test_dataset = config.get('data', {}).get('local', {}).get('test', 'data/test')
            
            # Jalankan evaluasi
            logger.info(f"üîç Mengevaluasi model dari {model_path}")
            results = evaluation_handler.evaluate(
                eval_type='regular',
                model_path=model_path,
                test_dataset=test_dataset,
                conf_threshold=conf_threshold
            )
            
            return results
    except Exception as e:
        logger.error(f"‚ùå Error saat evaluasi model: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def evaluate_multiple_runs(model_path=None, test_dataset=None, num_runs=3, conf_threshold=0.25):
    """
    Evaluasi model dengan multiple runs untuk konsistensi.
    
    Args:
        model_path: Path ke checkpoint model
        test_dataset: Path ke dataset testing
        num_runs: Jumlah run evaluasi
        conf_threshold: Threshold confidence
        
    Returns:
        Dict hasil evaluasi rata-rata
    """
    try:
        logger.info(f"üîÑ Menjalankan {num_runs} kali evaluasi untuk konsistensi")
        
        all_results = []
        for i in range(num_runs):
            logger.info(f"üîç Run {i+1}/{num_runs}")
            result = evaluate_model(model_path, test_dataset, conf_threshold)
            if result:
                all_results.append(result)
        
        if not all_results:
            logger.error("‚ùå Semua run evaluasi gagal")
            return None
            
        # Agregasi hasil
        avg_metrics = {}
        
        # Ambil metrik dasar dari hasil pertama
        base_metrics = all_results[0].get('metrics', {})
        for key, value in base_metrics.items():
            if isinstance(value, (int, float)) and not isinstance(value, bool):
                # Hitung rata-rata untuk nilai numerik
                values = [r.get('metrics', {}).get(key, 0) for r in all_results]
                avg_metrics[key] = sum(values) / len(values)
                # Tambahkan standar deviasi
                if len(values) > 1:
                    std = np.std(values)
                    avg_metrics[f"{key}_std"] = std
        
        # Buat hasil agregasi
        aggregated_result = {
            'metrics': avg_metrics,
            'num_runs': num_runs,
            'model_path': model_path,
            'test_dataset': test_dataset,
            'conf_threshold': conf_threshold,
            'original_results': all_results
        }
        
        # Tambahkan confusion matrix dari hasil terakhir jika ada
        if 'confusion_matrix' in all_results[-1]:
            aggregated_result['confusion_matrix'] = all_results[-1]['confusion_matrix']
            
        # Tambahkan sample detections jika ada
        if 'sample_detections' in all_results[-1]:
            aggregated_result['sample_detections'] = all_results[-1]['sample_detections']
        
        logger.success(f"‚úÖ Berhasil mengagregasi hasil dari {len(all_results)} run")
        return aggregated_result
        
    except Exception as e:
        logger.error(f"‚ùå Error saat menjalankan multiple evaluasi: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def visualize_evaluation_results(results, show_confusion_matrix=True, show_class_metrics=True):
    """
    Visualisasikan hasil evaluasi model menggunakan ModelVisualizer.
    
    Args:
        results: Dict hasil evaluasi dari evaluate_model()
        show_confusion_matrix: Apakah menampilkan confusion matrix
        show_class_metrics: Apakah menampilkan metrik per kelas
    """
    if not results:
        logger.warning("‚ö†Ô∏è Tidak ada hasil evaluasi untuk divisualisasikan")
        return
    
    try:
        # Gunakan visualizer bawaan jika tersedia
        if visualizer_available:
            if hasattr(visualizer, 'visualize_evaluation_results'):
                # ModelVisualizer method
                visualizer.visualize_evaluation_results(
                    results=results,
                    title="Hasil Evaluasi Model",
                    show_confusion_matrix=show_confusion_matrix,
                    show_class_metrics=show_class_metrics
                )
            elif hasattr(visualizer, 'display_evaluation_results'):
                # Alternative method name
                visualizer.display_evaluation_results(
                    results=results,
                    title="Hasil Evaluasi Model",
                    show_confusion_matrix=show_confusion_matrix,
                    show_class_metrics=show_class_metrics
                )
            else:
                logger.warning("‚ö†Ô∏è Visualizer tidak memiliki metode untuk menampilkan hasil evaluasi")
                _fallback_visualization(results, show_confusion_matrix, show_class_metrics)
        else:
            # Gunakan visualisasi fallback jika tidak ada visualizer
            _fallback_visualization(results, show_confusion_matrix, show_class_metrics)
            
    except Exception as e:
        logger.error(f"‚ùå Error saat visualisasi hasil evaluasi: {str(e)}")
        import traceback
        traceback.print_exc()
        
        # Fallback jika visualisasi error
        _fallback_visualization(results, show_confusion_matrix, show_class_metrics)

def _fallback_visualization(results, show_confusion_matrix=True, show_class_metrics=True):
    """
    Visualisasi fallback untuk menampilkan hasil evaluasi tanpa visualizer.
    
    Args:
        results: Dict hasil evaluasi
        show_confusion_matrix: Apakah menampilkan confusion matrix
        show_class_metrics: Apakah menampilkan metrik per kelas
    """
    try:
        # Dapatkan metrics
        metrics = results.get('metrics', {})
        
        # Tampilkan ringkasan metrics
        display(HTML("<h3>üìä Ringkasan Hasil Evaluasi</h3>"))
        
        # Format metrics untuk tampilan
        metrics_html = "<table style='width:60%; border-collapse:collapse; margin-bottom:20px;'>"
        metrics_html += "<tr style='background-color:#f0f0f0;'><th style='padding:8px; text-align:left; border:1px solid #ddd;'>Metrik</th><th style='padding:8px; text-align:right; border:1px solid #ddd;'>Nilai</th></tr>"
        
        # Key metrics to display
        key_metrics = {
            'accuracy': 'Akurasi',
            'precision': 'Precision',
            'recall': 'Recall',
            'f1': 'F1-Score',
            'mAP': 'mAP',
            'inference_time': 'Inference Time (ms)'
        }
        
        for key, label in key_metrics.items():
            if key in metrics:
                value = metrics[key]
                # Format based on metric type
                if key == 'inference_time':
                    formatted_value = f"{value:.2f} ms"
                elif key == 'mAP' or key in ['accuracy', 'precision', 'recall', 'f1']:
                    # Convert to percentage
                    formatted_value = f"{value * 100:.2f}%"
                else:
                    formatted_value = f"{value:.4f}"
                    
                # Add row with styling based on value quality
                if key in ['accuracy', 'precision', 'recall', 'f1', 'mAP']:
                    if value * 100 >= 90:
                        bg_color = "#d4edda"  # Good (green)
                    elif value * 100 >= 70:
                        bg_color = "#fff3cd"  # Average (yellow)
                    else:
                        bg_color = "#f8d7da"  # Poor (red)
                else:
                    bg_color = "#ffffff"  # Default
                    
                metrics_html += f"<tr style='background-color:{bg_color};'><td style='padding:8px; border:1px solid #ddd;'>{label}</td><td style='padding:8px; text-align:right; border:1px solid #ddd;'>{formatted_value}</td></tr>"
        
        metrics_html += "</table>"
        display(HTML(metrics_html))
        
        # Tampilkan std deviation jika ada
        std_metrics = {k: v for k, v in metrics.items() if k.endswith('_std')}
        if std_metrics:
            display(HTML("<h4>üìâ Standar Deviasi (Multiple Runs)</h4>"))
            
            std_html = "<table style='width:60%; border-collapse:collapse; margin-bottom:20px;'>"
            std_html += "<tr style='background-color:#f0f0f0;'><th style='padding:8px; text-align:left; border:1px solid #ddd;'>Metrik</th><th style='padding:8px; text-align:right; border:1px solid #ddd;'>Std Dev</th></tr>"
            
            for key, value in std_metrics.items():
                base_metric = key.replace('_std', '')
                label = key_metrics.get(base_metric, base_metric)
                
                if base_metric == 'inference_time':
                    formatted_value = f"{value:.2f} ms"
                elif base_metric in ['accuracy', 'precision', 'recall', 'f1', 'mAP']:
                    formatted_value = f"{value * 100:.2f}%"
                else:
                    formatted_value = f"{value:.4f}"
                
                std_html += f"<tr><td style='padding:8px; border:1px solid #ddd;'>{label}</td><td style='padding:8px; text-align:right; border:1px solid #ddd;'>{formatted_value}</td></tr>"
            
            std_html += "</table>"
            display(HTML(std_html))
        
        # Tampilkan confusion matrix jika diminta dan tersedia
        if show_confusion_matrix and 'confusion_matrix' in results:
            try:
                import matplotlib.pyplot as plt
                import seaborn as sns
                
                display(HTML("<h3>üîÑ Confusion Matrix</h3>"))
                
                cm = results['confusion_matrix']
                plt.figure(figsize=(10, 8))
                
                # Get class names
                class_names = results.get('class_names', [str(i) for i in range(cm.shape[0])])
                
                # Plot confusion matrix
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                            xticklabels=class_names, yticklabels=class_names)
                plt.title('Confusion Matrix')
                plt.ylabel('True Label')
                plt.xlabel('Predicted Label')
                plt.tight_layout()
                plt.show()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Tidak dapat menampilkan confusion matrix: {str(e)}")
        
        # Tampilkan metrik per kelas jika diminta
        if show_class_metrics:
            # Extract class metrics
            class_metrics = {}
            for key, value in metrics.items():
                if key.startswith(('precision_cls_', 'recall_cls_', 'f1_cls_')):
                    parts = key.split('_')
                    metric_type = parts[0]
                    class_id = int(parts[-1])
                    
                    if class_id not in class_metrics:
                        class_metrics[class_id] = {}
                    
                    class_metrics[class_id][metric_type] = value
            
            if class_metrics:
                try:
                    import pandas as pd
                    
                    display(HTML("<h3>üìä Metrik per Kelas</h3>"))
                    
                    # Get class names
                    class_names = results.get('class_names', {})
                    
                    # Prepare data for table
                    rows = []
                    for class_id, metrics in class_metrics.items():
                        class_name = class_names.get(class_id, f"Class {class_id}")
                        rows.append({
                            'Class': class_name,
                            'Precision': metrics.get('precision', 0) * 100,
                            'Recall': metrics.get('recall', 0) * 100,
                            'F1-Score': metrics.get('f1', 0) * 100
                        })
                    
                    # Create and display DataFrame
                    class_df = pd.DataFrame(rows)
                    display(class_df.style.format({
                        'Precision': '{:.2f}%',
                        'Recall': '{:.2f}%',
                        'F1-Score': '{:.2f}%'
                    }))
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Tidak dapat menampilkan metrik per kelas: {str(e)}")
        
        # Tampilkan informasi tambahan
        display(HTML("<h3>üìå Informasi Model</h3>"))
        
        model_info_html = "<table style='width:60%; border-collapse:collapse;'>"
        model_info_html += "<tr style='background-color:#f0f0f0;'><th style='padding:8px; text-align:left; border:1px solid #ddd;'>Parameter</th><th style='padding:8px; text-align:left; border:1px solid #ddd;'>Nilai</th></tr>"
        
        # Add model info rows
        model_info = [
            ('Backbone', config.get('model', {}).get('backbone', 'unknown')),
            ('Mode Deteksi', 'Multi-layer' if len(config.get('layers', [])) > 1 else 'Single-layer'),
            ('Ukuran Input', f"{config.get('model', {}).get('img_size', [0, 0])}")
        ]
        
        # Add dataset info
        test_dataset = results.get('test_dataset', 'unknown')
        model_info.append(('Dataset', test_dataset))
        
        # Add model path
        model_path = results.get('model_path', 'unknown')
        if isinstance(model_path, str):
            model_filename = os.path.basename(model_path)
            model_info.append(('Checkpoint', model_filename))
        
        # Add confidence threshold
        conf_threshold = results.get('conf_threshold', 0.25)
        model_info.append(('Confidence Threshold', f"{conf_threshold:.2f}"))
        
        # Add number of runs if multiple
        if 'num_runs' in results:
            model_info.append(('Jumlah Run', results['num_runs']))
        
        # Format model info table
        for label, value in model_info:
            model_info_html += f"<tr><td style='padding:8px; border:1px solid #ddd;'>{label}</td><td style='padding:8px; border:1px solid #ddd;'>{value}</td></tr>"
        
        model_info_html += "</table>"
        display(HTML(model_info_html))
        
        # Tampilkan sample detections jika tersedia
        if 'sample_detections' in results:
            display(HTML("<h3>üñºÔ∏è Contoh Hasil Deteksi</h3>"))
            
            try:
                import matplotlib.pyplot as plt
                
                samples = results['sample_detections']
                for i, sample in enumerate(samples[:3]):  # Limit to 3 samples
                    plt.figure(figsize=(10, 8))
                    plt.imshow(sample)
                    plt.axis('off')
                    plt.title(f"Sample Detection {i+1}")
                    plt.tight_layout()
                    plt.show()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Tidak dapat menampilkan contoh deteksi: {str(e)}")
    
    except Exception as e:
        logger.error(f"‚ùå Error pada fallback visualization: {str(e)}")

# Tampilkan UI
display(widgets.VBox([
    widgets.HTML("<h2>üìä Evaluasi Model</h2>"),
    widgets.HTML("<p>Jalankan evaluasi untuk menilai performa model setelah training.</p>"),
    model_dropdown,
    dataset_dropdown,
    evaluation_settings,
    run_evaluation_button,
    evaluation_output
]))