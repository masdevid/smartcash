{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📊 Persiapan Dataset SmartCash\n",
    "\n",
    "Notebook ini menjelaskan proses persiapan dataset untuk pelatihan model SmartCash.\n",
    "\n",
    "## 📋 Daftar Isi\n",
    "1. [Setup Environment](#setup)\n",
    "2. [Persiapan Dataset](#dataset)\n",
    "3. [Validasi Dataset](#validasi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment <a id='setup'></a>\n",
    "\n",
    "Pertama, kita perlu setup environment dan import library yang diperlukan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Project root: /Users/masdevid/Projects/smartcash\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path().absolute().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "# Import required modules\n",
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify project structure\n",
    "required_dirs = ['configs', 'data', 'smartcash']\n",
    "missing_dirs = [d for d in required_dirs if not (project_root / d).exists()]\n",
    "\n",
    "if missing_dirs:\n",
    "    raise RuntimeError(\n",
    "        f\"Missing required directories: {missing_dirs}\\n\"\n",
    "        f\"Please run this notebook from the 'notebooks' directory\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Configuration\n",
    "\n",
    "Load konfigurasi dari file `base_config.yaml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded config from: /Users/masdevid/Projects/smartcash/configs/base_config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = project_root / 'configs' / 'base_config.yaml'\n",
    "\n",
    "if not config_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Config file not found: {config_path}\\n\"\n",
    "        f\"Please create base_config.yaml in the configs directory\"\n",
    "    )\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"✅ Loaded config from: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Import SmartCash Modules\n",
    "\n",
    "Import modul-modul yang diperlukan dari SmartCash:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported SmartCash modules\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from smartcash.utils.logger import SmartCashLogger\n",
    "    from smartcash.handlers.data_handler import DataHandler\n",
    "    from smartcash.handlers.roboflow_handler import RoboflowHandler\n",
    "    from smartcash.utils.preprocessing import ImagePreprocessor\n",
    "    \n",
    "    print(\"✅ Successfully imported SmartCash modules\")\n",
    "except ImportError as e:\n",
    "    raise ImportError(\n",
    "        f\"Failed to import SmartCash modules: {str(e)}\\n\"\n",
    "        f\"Please make sure all required modules are installed\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Persiapan Dataset <a id='dataset'></a>\n",
    "\n",
    "Setup data handler dan mulai persiapan dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-20 15:54:49 - ℹ️ Data directories setup complete:\n",
      "Raw data: /Users/masdevid/Projects/smartcash/data/raw\n",
      "Processed data: /Users/masdevid/Projects/smartcash/data/processed\n"
     ]
    }
   ],
   "source": [
    "# Initialize logger\n",
    "logger = SmartCashLogger('dataset_preparation')\n",
    "\n",
    "# Setup data directories\n",
    "data_dir = project_root / 'data'\n",
    "raw_dir = data_dir / 'raw'\n",
    "processed_dir = data_dir / 'processed'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Data directories setup complete:\\n\"\n",
    "           f\"Raw data: {raw_dir}\\n\"\n",
    "           f\"Processed data: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load Dataset\n",
    "\n",
    "Pilih sumber dataset (lokal atau Roboflow):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-20 15:54:49 - \u001b[32m✅ Dataset loaded from: /Users/masdevid/Projects/smartcash/data/raw\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Choose data source\n",
    "USE_ROBOFLOW = False  # Set True to use Roboflow\n",
    "\n",
    "try:\n",
    "    if USE_ROBOFLOW:\n",
    "        # Check API key\n",
    "        api_key = os.getenv('ROBOFLOW_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\n",
    "                \"ROBOFLOW_API_KEY not found in environment variables\\n\"\n",
    "                \"Please set it in .env file\"\n",
    "            )\n",
    "            \n",
    "        handler = RoboflowHandler(\n",
    "            config_path=str(config_path),\n",
    "            data_dir=str(raw_dir),\n",
    "            api_key=api_key,\n",
    "            logger=logger\n",
    "        )\n",
    "        source_dir = handler.download_dataset()\n",
    "    else:\n",
    "        handler = DataHandler(\n",
    "            config_path=str(config_path),\n",
    "            data_dir=str(raw_dir),\n",
    "            logger=logger\n",
    "        )\n",
    "        source_dir = str(raw_dir)\n",
    "        \n",
    "    logger.success(f\"Dataset loaded from: {source_dir}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load dataset: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Process Dataset\n",
    "\n",
    "Proses dataset untuk setiap split data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 validation error for InitSchema\n",
      "size\n",
      "  Input should be a valid tuple [type=tuple_type, input_value=640, input_type=int]\n",
      "    For further information visit https://errors.pydantic.dev/2.10/v/tuple_type\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "6 validation errors for InitSchema\np\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nscale\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nratio\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsize\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ninterpolation\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nmask_interpolation\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize preprocessor\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m \u001b[43mImagePreprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Process each split\u001b[39;00m\n\u001b[1;32m      8\u001b[0m splits \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Projects/smartcash/smartcash/utils/preprocessing.py:35\u001b[0m, in \u001b[0;36mImagePreprocessor.__init__\u001b[0;34m(self, config_path, logger, cache_size_gb)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_size\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Setup komponen\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugmentor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_augmentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoord_normalizer \u001b[38;5;241m=\u001b[39m CoordinateNormalizer(logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m PreprocessingCache(\n\u001b[1;32m     38\u001b[0m     max_size_gb\u001b[38;5;241m=\u001b[39mcache_size_gb,\n\u001b[1;32m     39\u001b[0m     logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\n\u001b[1;32m     40\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/smartcash/smartcash/utils/preprocessing.py:68\u001b[0m, in \u001b[0;36mImagePreprocessor._setup_augmentations\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Setup pipeline augmentasi data\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m aug_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m---> 68\u001b[0m     \u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRandomResizedCrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     73\u001b[0m     A\u001b[38;5;241m.\u001b[39mHorizontalFlip(p\u001b[38;5;241m=\u001b[39maug_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfliplr\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     74\u001b[0m     A\u001b[38;5;241m.\u001b[39mVerticalFlip(p\u001b[38;5;241m=\u001b[39maug_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflipud\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     75\u001b[0m     A\u001b[38;5;241m.\u001b[39mHueSaturationValue(\n\u001b[1;32m     76\u001b[0m         hue_shift_limit\u001b[38;5;241m=\u001b[39maug_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhsv_h\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     77\u001b[0m         sat_shift_limit\u001b[38;5;241m=\u001b[39maug_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhsv_s\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     78\u001b[0m         val_shift_limit\u001b[38;5;241m=\u001b[39maug_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhsv_v\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     79\u001b[0m         p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     80\u001b[0m     ),\n\u001b[1;32m     81\u001b[0m     A\u001b[38;5;241m.\u001b[39mShiftScaleRotate(\n\u001b[1;32m     82\u001b[0m         shift_limit\u001b[38;5;241m=\u001b[39maug_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     83\u001b[0m         scale_limit\u001b[38;5;241m=\u001b[39maug_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     84\u001b[0m         rotate_limit\u001b[38;5;241m=\u001b[39maug_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdegrees\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     85\u001b[0m         p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m     86\u001b[0m     ),\n\u001b[1;32m     87\u001b[0m     A\u001b[38;5;241m.\u001b[39mNormalize(\n\u001b[1;32m     88\u001b[0m         mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[1;32m     89\u001b[0m         std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m],\n\u001b[1;32m     90\u001b[0m         p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     91\u001b[0m     )\n\u001b[1;32m     92\u001b[0m ])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/smartcash/lib/python3.12/site-packages/albumentations/core/validation.py:48\u001b[0m, in \u001b[0;36mValidatedTransformMeta.__new__.<locals>.custom_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m warn(\u001b[38;5;28mstr\u001b[39m(e), stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Use default values for invalid parameters\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mdct\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mInitSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m validated_kwargs \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[1;32m     50\u001b[0m validated_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Also remove from default values\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/smartcash/lib/python3.12/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 6 validation errors for InitSchema\np\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nscale\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nratio\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nsize\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\ninterpolation\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing\nmask_interpolation\n  Field required [type=missing, input_value={}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/missing"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = ImagePreprocessor(\n",
    "    config_path=str(config_path),\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Process each split\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "for split in splits:\n",
    "    logger.info(f\"Processing {split} split...\")\n",
    "    \n",
    "    try:\n",
    "        # Setup split directories\n",
    "        split_dir = Path(source_dir) / split\n",
    "        out_dir = processed_dir / split\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get image and label files\n",
    "        image_dir = split_dir / 'images'\n",
    "        label_dir = split_dir / 'labels'\n",
    "        \n",
    "        if not image_dir.exists() or not label_dir.exists():\n",
    "            logger.warning(f\"Skipping {split}: directories not found\")\n",
    "            continue\n",
    "            \n",
    "        image_files = sorted(image_dir.glob('*.jpg'))\n",
    "        label_files = sorted(label_dir.glob('*.txt'))\n",
    "        \n",
    "        # Process each file\n",
    "        for img_path, lbl_path in zip(image_files, label_files):\n",
    "            try:\n",
    "                # Process and save\n",
    "                preprocessor.process_image_and_label(\n",
    "                    image_path=str(img_path),\n",
    "                    label_path=str(lbl_path),\n",
    "                    save_dir=str(out_dir),\n",
    "                    augment=(split == 'train')\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to process {img_path.name}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        logger.success(f\"Completed processing {split} split\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process {split} split: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validasi Dataset <a id='validasi'></a>\n",
    "\n",
    "Validasi hasil preprocessing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate processed dataset\n",
    "validation_results = {}\n",
    "\n",
    "for split in splits:\n",
    "    logger.info(f\"Validating {split} split...\")\n",
    "    \n",
    "    try:\n",
    "        # Get processed directories\n",
    "        split_dir = processed_dir / split\n",
    "        image_dir = split_dir / 'images'\n",
    "        label_dir = split_dir / 'labels'\n",
    "        \n",
    "        # Check directories exist\n",
    "        if not image_dir.exists() or not label_dir.exists():\n",
    "            raise FileNotFoundError(f\"Missing directories for {split} split\")\n",
    "            \n",
    "        # Count files\n",
    "        image_files = list(image_dir.glob('*.jpg'))\n",
    "        label_files = list(label_dir.glob('*.txt'))\n",
    "        \n",
    "        # Store results\n",
    "        validation_results[split] = {\n",
    "            'images': len(image_files),\n",
    "            'labels': len(label_files),\n",
    "            'status': 'OK' if len(image_files) == len(label_files) else 'ERROR'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        validation_results[split] = {\n",
    "            'images': 0,\n",
    "            'labels': 0,\n",
    "            'status': f'ERROR: {str(e)}'\n",
    "        }\n",
    "\n",
    "# Print summary\n",
    "logger.info(\"\\nDataset Validation Summary:\")\n",
    "for split, result in validation_results.items():\n",
    "    status_color = 'green' if result['status'] == 'OK' else 'red'\n",
    "    logger.info(\n",
    "        f\"{split}: {result['images']} images, {result['labels']} labels \"\n",
    "        f\"[{result['status']}]\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smartcash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
