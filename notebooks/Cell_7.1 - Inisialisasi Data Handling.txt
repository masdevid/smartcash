# Cell 7.1: Inisialisasi dan Import Data Handling - Setup awal untuk pengolahan dan manajemen dataset

# ===== 1. IMPORT DAN SETUP AWAL =====
import ipywidgets as widgets
from IPython.display import display, HTML, clear_output
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
import gc
import sys
from pathlib import Path
import torch
import pickle
from contextlib import contextmanager
from typing import Dict, Any, Optional

# Tambahkan path ke PYTHONPATH jika belum ada
if os.getcwd() not in sys.path:
    sys.path.append(os.getcwd())

# Import modul yang diperlukan
from smartcash.utils.logger import get_logger, SmartCashLogger
from smartcash.handlers.data_manager import DataManager

# ===== 2. UTILITY FUNCTIONS =====
@contextmanager
def memory_manager():
    """Context manager untuk mengoptimalkan penggunaan memori."""
    try:
        yield
    finally:
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

def safe_load_config(config_path: str = 'config.pkl') -> Dict[str, Any]:
    """Load config dengan penanganan error."""
    try:
        with open(config_path, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        logger.error(f"Error loading config: {str(e)}")
        raise

# ===== 3. MAIN INITIALIZATION =====
# Set logger yang kompatibel dengan Colab
logger = get_logger("data_pipeline", log_to_colab=True, log_to_file=True)

with memory_manager():
    # Load config
    config = safe_load_config()
    
    # Initialize data manager
    data_manager = DataManager(
        config=config,
        logger=logger
    )
    
    # Get class names and dataset info
    class_names = data_manager.get_class_names()
    logger.info(f"Loaded {len(class_names)} classes: {class_names}")
    
    # Get dataset statistics
    stats = data_manager.get_dataset_stats()
    logger.info("\nDataset statistics:")
    for split, split_stats in stats.items():
        logger.info(f"{split.capitalize()}:")
        for key, value in split_stats.items():
            logger.info(f"  {key}: {value}")

# ===== 4. DATA LOADERS SETUP =====
def create_dataloaders(data_manager: DataManager, config: Dict[str, Any]) -> Dict[str, Any]:
    """Create optimized data loaders with error handling."""
    dataloaders = {}
    try:
        dataloaders['train'] = data_manager.get_dataloader(
            data_path=config['data']['local']['train'],
            batch_size=config['model']['batch_size'],
            mode='train',
            num_workers=config['model']['workers'],
            pin_memory=torch.cuda.is_available()  # Optimize GPU transfer
        )

        dataloaders['val'] = data_manager.get_dataloader(
            data_path=config['data']['local']['valid'],
            batch_size=config['model']['batch_size'],
            mode='val',
            num_workers=config['model']['workers'],
            pin_memory=torch.cuda.is_available()
        )

        dataloaders['test'] = data_manager.get_dataloader(
            data_path=config['data']['local']['test'],
            batch_size=config['model']['batch_size'],
            mode='test',
            num_workers=1,  # Reduce workers for testing
            pin_memory=torch.cuda.is_available()
        )
        
        return dataloaders
    except Exception as e:
        logger.error(f"Error creating dataloaders: {str(e)}")
        raise

# Create and save dataloaders
with memory_manager():
    dataloaders = create_dataloaders(data_manager, config)
    try:
        with open('dataloaders.pkl', 'wb') as f:
            pickle.dump(dataloaders, f)
        logger.success("âœ¨ Data handling initialization completed!")
    except Exception as e:
        logger.error(f"Error saving dataloaders: {str(e)}")
        raise