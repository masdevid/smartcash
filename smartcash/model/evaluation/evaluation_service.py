"""
File: smartcash/model/evaluation/evaluation_service.py
Deskripsi: Main evaluation orchestrator untuk research scenarios dengan progress tracking
"""

import time
from pathlib import Path
from typing import Dict, Any, List, Optional
import torch
import numpy as np
from PIL import Image

from smartcash.common.logger import get_logger
from smartcash.model.evaluation.scenario_manager import ScenarioManager
from smartcash.model.evaluation.evaluation_metrics import EvaluationMetrics
from smartcash.model.evaluation.checkpoint_selector import CheckpointSelector
from smartcash.model.evaluation.utils.evaluation_progress_bridge import EvaluationProgressBridge
from smartcash.model.evaluation.utils.inference_timer import InferenceTimer
from smartcash.model.evaluation.utils.results_aggregator import ResultsAggregator

class EvaluationService:
    """Main evaluation service untuk research scenarios dengan comprehensive metrics"""
    
    def __init__(self, model_api=None, config: Dict[str, Any] = None):
        self.config = config or {}
        self.logger = get_logger('evaluation_service')
        self.model_api = model_api
        
        # Initialize components
        self.scenario_manager = ScenarioManager(config)
        self.evaluation_metrics = EvaluationMetrics(config)
        self.checkpoint_selector = CheckpointSelector(config=config)
        self.inference_timer = InferenceTimer(config)
        self.results_aggregator = ResultsAggregator(config)
        
        # Progress tracking
        self.progress_bridge = None
        self.ui_components = {}
        
    def run_evaluation(self, scenarios: List[str] = None, checkpoints: List[str] = None,
                      progress_callback=None, metrics_callback=None, 
                      ui_components: Dict[str, Any] = None) -> Dict[str, Any]:
        """ðŸš€ Run comprehensive evaluation pipeline"""
        
        # Setup progress tracking
        self.ui_components = ui_components or {}
        self.progress_bridge = EvaluationProgressBridge(self.ui_components, progress_callback)
        
        # Default scenarios
        if scenarios is None:
            scenarios = ['position_variation', 'lighting_variation']
        
        # Get available checkpoints
        if checkpoints is None:
            available_checkpoints = self.checkpoint_selector.list_available_checkpoints()
            checkpoints = [cp['path'] for cp in available_checkpoints[:2]]  # Top 2 checkpoints
        
        self.progress_bridge.start_evaluation(scenarios, checkpoints, "Research Evaluation")
        
        try:
            evaluation_results = {}
            
            # Prepare scenarios
            self.logger.info(f"ðŸŽ¯ Preparing {len(scenarios)} scenarios")
            scenario_preparation = self.scenario_manager.prepare_all_scenarios()
            
            for scenario_idx, scenario_name in enumerate(scenarios):
                self.progress_bridge.update_scenario(scenario_idx, scenario_name, "Preparing scenario data")
                
                scenario_results = {}
                
                for checkpoint_idx, checkpoint_path in enumerate(checkpoints):
                    self.progress_bridge.update_checkpoint(checkpoint_idx, Path(checkpoint_path).name, "Loading checkpoint")
                    
                    # Load checkpoint
                    checkpoint_info = self._load_checkpoint(checkpoint_path)
                    if not checkpoint_info:
                        continue
                    
                    # Run scenario evaluation
                    scenario_result = self._evaluate_scenario(
                        scenario_name, checkpoint_info, 
                        checkpoint_idx, len(checkpoints)
                    )
                    
                    # Store results
                    backbone = checkpoint_info.get('backbone', 'unknown')
                    scenario_results[backbone] = {
                        'checkpoint_info': checkpoint_info,
                        'metrics': scenario_result['metrics'],
                        'additional_data': scenario_result.get('additional_data', {})
                    }
                    
                    # Add to aggregator
                    self.results_aggregator.add_scenario_results(
                        scenario_name, backbone, checkpoint_info,
                        scenario_result['metrics'], scenario_result.get('additional_data', {})
                    )
                
                evaluation_results[scenario_name] = scenario_results
            
            # Generate comprehensive summary
            self.progress_bridge.update_metrics(90, "Generating comprehensive summary")
            summary = self._generate_evaluation_summary(evaluation_results)
            
            # Export results
            self.progress_bridge.update_metrics(95, "Exporting results")
            export_files = self.results_aggregator.export_results()
            
            self.progress_bridge.complete_evaluation("Evaluation completed successfully!")
            
            final_results = {
                'status': 'success',
                'evaluation_results': evaluation_results,
                'summary': summary,
                'export_files': export_files,
                'scenarios_evaluated': len(scenarios),
                'checkpoints_evaluated': len(checkpoints)
            }
            
            # Call metrics callback
            if metrics_callback:
                metrics_callback(final_results)
            
            return final_results
            
        except Exception as e:
            error_msg = f"Evaluation failed: {str(e)}"
            self.progress_bridge.evaluation_error(error_msg)
            self.logger.error(f"âŒ {error_msg}")
            
            return {
                'status': 'error',
                'error': error_msg,
                'partial_results': getattr(self, 'evaluation_results', {})
            }
    
    def run_scenario(self, scenario_name: str, checkpoint_path: str) -> Dict[str, Any]:
        """ðŸŽ¯ Run single scenario evaluation"""
        
        self.logger.info(f"ðŸŽ¯ Running {scenario_name} evaluation")
        
        # Load checkpoint
        checkpoint_info = self._load_checkpoint(checkpoint_path)
        if not checkpoint_info:
            return {'status': 'error', 'error': 'Failed to load checkpoint'}
        
        # Evaluate scenario
        result = self._evaluate_scenario(scenario_name, checkpoint_info, 0, 1)
        
        return {
            'status': 'success',
            'scenario_name': scenario_name,
            'checkpoint_info': checkpoint_info,
            'metrics': result['metrics'],
            'additional_data': result.get('additional_data', {})
        }
    
    def load_checkpoint(self, checkpoint_path: str) -> Dict[str, Any]:
        """ðŸ“¥ Load dan validate checkpoint"""
        return self._load_checkpoint(checkpoint_path)
    
    def compute_metrics(self, predictions: List[Dict], ground_truths: List[Dict], 
                       inference_times: List[float] = None) -> Dict[str, Any]:
        """ðŸ“Š Compute comprehensive metrics"""
        return self.evaluation_metrics.get_metrics_summary(predictions, ground_truths, inference_times)
    
    def generate_report(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ“‹ Generate evaluation report"""
        return self._generate_evaluation_summary(evaluation_results)
    
    def compare_scenarios(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """âš–ï¸ Compare scenarios performance"""
        return self.results_aggregator.compare_backbones()
    
    def save_results(self, results: Dict[str, Any], formats: List[str] = None) -> Dict[str, str]:
        """ðŸ’¾ Save evaluation results"""
        
        # Add results to aggregator if not already there
        if 'evaluation_results' in results:
            for scenario_name, backbone_results in results['evaluation_results'].items():
                for backbone, result_data in backbone_results.items():
                    self.results_aggregator.add_scenario_results(
                        scenario_name, backbone,
                        result_data.get('checkpoint_info', {}),
                        result_data.get('metrics', {}),
                        result_data.get('additional_data', {})
                    )
        
        return self.results_aggregator.export_results(formats)
    
    def _load_checkpoint(self, checkpoint_path: str) -> Optional[Dict[str, Any]]:
        """ðŸ“¥ Load checkpoint dengan validation"""
        try:
            checkpoint_info = self.checkpoint_selector.select_checkpoint(checkpoint_path)
            
            if self.model_api:
                # Load model dengan checkpoint
                load_result = self.model_api.load_checkpoint(checkpoint_path)
                if load_result.get('success', False):
                    checkpoint_info['model_loaded'] = True
                    self.logger.info(f"âœ… Model loaded: {checkpoint_info['display_name']}")
                else:
                    self.logger.warning(f"âš ï¸ Model load failed: {checkpoint_path}")
                    checkpoint_info['model_loaded'] = False
            
            return checkpoint_info
            
        except Exception as e:
            self.logger.error(f"âŒ Checkpoint load error: {str(e)}")
            return None
    
    def _evaluate_scenario(self, scenario_name: str, checkpoint_info: Dict[str, Any], 
                          checkpoint_idx: int, total_checkpoints: int) -> Dict[str, Any]:
        """ðŸ§ª Evaluate single scenario dengan specific checkpoint"""
        
        # Get scenario data path
        scenario_dir = self.scenario_manager.evaluation_dir / scenario_name
        
        if not scenario_dir.exists():
            # Generate scenario data if missing
            self.progress_bridge.update_substep(f"Generating {scenario_name} data")
            self.scenario_manager.generate_scenario_data(scenario_name)
        
        # Load test data
        self.progress_bridge.update_metrics(10, f"Loading {scenario_name} test data")
        test_data = self._load_scenario_data(scenario_dir)
        
        if not test_data['images'] or not test_data['labels']:
            raise ValueError(f"No test data found untuk {scenario_name}")
        
        # Run inference dengan timing
        self.progress_bridge.update_metrics(30, "Running inference")
        predictions, inference_times = self._run_inference_with_timing(
            test_data['images'], checkpoint_info
        )
        
        # Calculate metrics
        self.progress_bridge.update_metrics(70, "Calculating metrics")
        metrics = self.evaluation_metrics.get_metrics_summary(
            predictions, test_data['labels'], inference_times
        )
        
        # Additional analysis
        self.progress_bridge.update_metrics(90, "Performing additional analysis")
        additional_data = self._perform_additional_analysis(
            predictions, test_data['labels'], scenario_name
        )
        
        self.logger.info(f"âœ… {scenario_name} evaluation complete: mAP={metrics.get('mAP', 0):.3f}")
        
        return {
            'metrics': metrics,
            'additional_data': additional_data,
            'test_data_count': len(test_data['images'])
        }
    
    def _load_scenario_data(self, scenario_dir: Path) -> Dict[str, List]:
        """ðŸ“ Load scenario test data"""
        
        images_dir = scenario_dir / 'images'
        labels_dir = scenario_dir / 'labels'
        
        # Load images
        image_files = []
        for ext in ['.jpg', '.jpeg', '.png']:
            image_files.extend(list(images_dir.glob(f'*{ext}')))
        
        # Load corresponding labels
        labels = []
        images = []
        
        for img_file in image_files:
            label_file = labels_dir / f"{img_file.stem}.txt"
            
            if label_file.exists():
                # Load image
                try:
                    img = Image.open(img_file).convert('RGB')
                    images.append({
                        'image': img,
                        'filename': img_file.name,
                        'path': str(img_file)
                    })
                    
                    # Load label
                    annotations = []
                    with open(label_file, 'r') as f:
                        for line in f:
                            parts = line.strip().split()
                            if len(parts) >= 5:
                                class_id = int(float(parts[0]))
                                bbox = [float(x) for x in parts[1:5]]
                                annotations.append({
                                    'class_id': class_id,
                                    'bbox': bbox
                                })
                    
                    labels.append({
                        'filename': img_file.name,
                        'annotations': annotations
                    })
                    
                except Exception as e:
                    self.logger.debug(f"âš ï¸ Error loading {img_file.name}: {str(e)}")
        
        self.logger.info(f"ðŸ“ Loaded {len(images)} test images dengan labels")
        return {'images': images, 'labels': labels}
    
    def _run_inference_with_timing(self, test_images: List[Dict], checkpoint_info: Dict[str, Any]) -> tuple:
        """â±ï¸ Run inference dengan timing measurement"""
        
        predictions = []
        inference_times = []
        
        # Warmup jika model tersedia
        if self.model_api and checkpoint_info.get('model_loaded', False):
            self.logger.info("ðŸ”¥ Warming up model")
            # Create dummy input untuk warmup
            dummy_input = torch.randn(1, 3, 640, 640)
            if torch.cuda.is_available():
                dummy_input = dummy_input.cuda()
            
            warmup_result = self.inference_timer.warmup_model(
                self.model_api.model, dummy_input, warmup_runs=5
            )
        
        # Process each image
        for idx, img_data in enumerate(test_images):
            
            if self.model_api and checkpoint_info.get('model_loaded', False):
                # Real inference dengan model
                with self.inference_timer.time_inference(batch_size=1, operation='evaluation'):
                    try:
                        # Convert PIL to tensor
                        img_tensor = self._preprocess_image(img_data['image'])
                        
                        # Run prediction
                        pred_result = self.model_api.predict(img_tensor)
                        
                        detections = []
                        if pred_result.get('success', False):
                            # Convert predictions to standard format
                            for detection in pred_result.get('detections', []):
                                detections.append({
                                    'class_id': detection.get('class_id', 0),
                                    'confidence': detection.get('confidence', 0),
                                    'bbox': detection.get('bbox', [0, 0, 0, 0])
                                })
                        
                        predictions.append({
                            'filename': img_data['filename'],
                            'detections': detections
                        })
                        
                        # Record timing
                        if self.inference_timer.timings['evaluation']:
                            last_timing = self.inference_timer.timings['evaluation'][-1]
                            inference_times.append(last_timing['time'])
                        
                    except Exception as e:
                        self.logger.debug(f"âš ï¸ Inference error untuk {img_data['filename']}: {str(e)}")
                        predictions.append({
                            'filename': img_data['filename'],
                            'detections': []
                        })
                        inference_times.append(0.1)  # Default timing
            
            else:
                # Fallback: simulate inference untuk testing
                import random
                time.sleep(0.01)  # Simulate processing time
                
                # Mock predictions
                mock_detections = []
                for _ in range(random.randint(0, 3)):
                    mock_detections.append({
                        'class_id': random.randint(0, 6),
                        'confidence': random.uniform(0.3, 0.9),
                        'bbox': [random.uniform(0.1, 0.8), random.uniform(0.1, 0.8),
                                random.uniform(0.1, 0.3), random.uniform(0.1, 0.3)]
                    })
                
                predictions.append({
                    'filename': img_data['filename'],
                    'detections': mock_detections
                })
                inference_times.append(random.uniform(0.05, 0.15))
            
            # Update progress
            if idx % 10 == 0:
                progress = 30 + (idx / len(test_images)) * 40  # 30-70% range
                self.progress_bridge.update_metrics(int(progress), f"Processing image {idx + 1}/{len(test_images)}")
        
        self.logger.info(f"â±ï¸ Inference complete: {len(predictions)} predictions, avg time: {np.mean(inference_times):.3f}s")
        return predictions, inference_times
    
    def _preprocess_image(self, img: Image.Image) -> torch.Tensor:
        """ðŸ–¼ï¸ Preprocess image untuk inference"""
        # Resize to model input size
        img_resized = img.resize((640, 640))
        
        # Convert to tensor dan normalize
        img_array = np.array(img_resized).astype(np.float32) / 255.0
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).unsqueeze(0)
        
        # Move to GPU jika tersedia
        if torch.cuda.is_available() and self.model_api:
            img_tensor = img_tensor.cuda()
        
        return img_tensor
    
    def _perform_additional_analysis(self, predictions: List[Dict], ground_truths: List[Dict], 
                                   scenario_name: str) -> Dict[str, Any]:
        """ðŸ” Perform additional analysis specific to scenario"""
        
        additional_data = {
            'scenario_name': scenario_name,
            'total_predictions': len(predictions),
            'total_ground_truths': len(ground_truths),
            'analysis_timestamp': time.time()
        }
        
        # Currency denomination analysis jika enabled
        currency_config = self.config.get('analysis', {}).get('currency_analysis', {})
        if currency_config.get('enabled', True):
            additional_data['currency_analysis'] = self._analyze_currency_detection(
                predictions, ground_truths, currency_config
            )
        
        # Class distribution analysis
        class_config = self.config.get('analysis', {}).get('class_analysis', {})
        if class_config.get('enabled', True):
            additional_data['class_distribution'] = self._analyze_class_distribution(
                predictions, ground_truths
            )
        
        return additional_data
    
    def _analyze_currency_detection(self, predictions: List[Dict], ground_truths: List[Dict], 
                                  config: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ’° Analyze currency denomination detection"""
        
        primary_layer = config.get('primary_layer', 'banknote')
        confidence_threshold = config.get('confidence_threshold', 0.3)
        
        analysis = {
            'primary_layer': primary_layer,
            'confidence_threshold': confidence_threshold,
            'correct_denominations': 0,
            'total_detections': 0,
            'denomination_accuracy': 0.0
        }
        
        # Analyze currency detection accuracy
        for pred, gt in zip(predictions, ground_truths):
            pred_detections = [d for d in pred.get('detections', []) 
                             if d.get('confidence', 0) >= confidence_threshold]
            gt_annotations = gt.get('annotations', [])
            
            analysis['total_detections'] += len(pred_detections)
            
            # Check if primary denominations match
            for pred_det in pred_detections:
                pred_class = pred_det.get('class_id', -1)
                
                # Find matching ground truth dalam main banknote classes (0-6)
                if pred_class in range(7):  # Main banknote classes
                    for gt_ann in gt_annotations:
                        gt_class = gt_ann.get('class_id', -1)
                        if gt_class == pred_class:
                            analysis['correct_denominations'] += 1
                            break
        
        # Calculate denomination accuracy
        if analysis['total_detections'] > 0:
            analysis['denomination_accuracy'] = analysis['correct_denominations'] / analysis['total_detections']
        
        return analysis
    
    def _analyze_class_distribution(self, predictions: List[Dict], ground_truths: List[Dict]) -> Dict[str, Any]:
        """ðŸ“Š Analyze class distribution dalam predictions vs ground truth"""
        
        pred_distribution = {}
        gt_distribution = {}
        
        # Count predictions
        for pred in predictions:
            for detection in pred.get('detections', []):
                class_id = detection.get('class_id', -1)
                pred_distribution[class_id] = pred_distribution.get(class_id, 0) + 1
        
        # Count ground truths
        for gt in ground_truths:
            for annotation in gt.get('annotations', []):
                class_id = annotation.get('class_id', -1)
                gt_distribution[class_id] = gt_distribution.get(class_id, 0) + 1
        
        return {
            'predictions_distribution': pred_distribution,
            'ground_truth_distribution': gt_distribution,
            'classes_detected': len(pred_distribution),
            'classes_in_ground_truth': len(gt_distribution)
        }
    
    def _generate_evaluation_summary(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ“‹ Generate comprehensive evaluation summary"""
        
        # Aggregate results
        aggregated = self.results_aggregator.aggregate_metrics()
        
        # Generate summary
        summary = self.results_aggregator.generate_summary()
        
        return {
            'evaluation_overview': summary.get('evaluation_overview', {}),
            'aggregated_metrics': aggregated,
            'key_findings': summary.get('key_findings', []),
            'recommendations': summary.get('recommendations', []),
            'best_configurations': aggregated.get('best_configurations', {}),
            'backbone_comparison': aggregated.get('backbone_comparison', {}),
            'scenario_comparison': aggregated.get('scenario_comparison', {})
        }


# Factory functions
def create_evaluation_service(model_api=None, config: Dict[str, Any] = None) -> EvaluationService:
    """ðŸ­ Factory untuk EvaluationService"""
    return EvaluationService(model_api, config)

def run_evaluation_pipeline(scenarios: List[str] = None, checkpoints: List[str] = None,
                           model_api=None, config: Dict[str, Any] = None,
                           progress_callback=None, ui_components: Dict[str, Any] = None) -> Dict[str, Any]:
    """ðŸš€ One-liner untuk run complete evaluation pipeline"""
    service = create_evaluation_service(model_api, config)
    return service.run_evaluation(scenarios, checkpoints, progress_callback, ui_components=ui_components)