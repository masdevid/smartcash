#!/usr/bin/env python3
"""
Validation batch processing for the unified training pipeline.

This module handles processing of individual validation batches,
including loss computation and data collection.
"""

import torch

from smartcash.common.logger import get_logger
from .prediction_processor import PredictionProcessor

logger = get_logger(__name__)


class ValidationBatchProcessor:
    """Handles processing of individual validation batches."""
    
    def __init__(self, model, config, prediction_processor: PredictionProcessor):
        """
        Initialize validation batch processor.
        
        Args:
            model: PyTorch model
            config: Training configuration
            prediction_processor: Prediction processor instance
        """
        self.model = model
        self.config = config
        self.prediction_processor = prediction_processor
    
    def process_batch(self, images, targets, loss_manager, batch_idx, num_batches,
                     phase_num, all_predictions, all_targets):
        """
        Process a single validation batch.
        
        Args:
            images: Input images tensor
            targets: Target labels tensor
            loss_manager: Loss manager instance
            batch_idx: Current batch index
            num_batches: Total number of batches
            phase_num: Current training phase
            all_predictions: Dictionary to collect predictions
            all_targets: Dictionary to collect targets
            
        Returns:
            Dictionary containing batch metrics
        """
        if batch_idx == 0 or batch_idx % 10 == 0:  # Log first and every 10th batch
            logger.debug(f"Processing validation batch {batch_idx+1}/{num_batches}, images: {images.shape}, targets: {targets.shape}")
        
        device = next(self.model.parameters()).device
        
        # Optimized non-blocking transfers
        images = images.to(device, non_blocking=True)
        targets = targets.to(device, non_blocking=True)
        
        # For first batch, ensure transfer is complete to avoid later sync overhead
        if batch_idx == 0:
            import torch
            if torch.cuda.is_available():
                torch.cuda.synchronize()
        
        # Get model predictions
        predictions = self.model(images)
        
        if batch_idx == 0:  # Only log on first batch to reduce noise
            logger.debug(f"Model predictions type: {type(predictions)}, structure: {type(predictions).__name__}")
            if isinstance(predictions, (list, tuple)):
                logger.debug(f"  Predictions list length: {len(predictions)}")
                if len(predictions) > 0:
                    logger.debug(f"  First prediction type: {type(predictions[0])}, shape: {getattr(predictions[0], 'shape', 'N/A')}")
        
        # Normalize predictions format
        predictions = self.prediction_processor.normalize_validation_predictions(
            predictions, phase_num, batch_idx
        )
        
        # Store for mAP processing
        self.prediction_processor.last_normalized_predictions = predictions
        
        # Compute loss
        try:
            loss, loss_breakdown = loss_manager.compute_loss(predictions, targets, images.shape[-1])
            
            # Debug: Check if loss is zero
            if loss.item() == 0.0:
                logger.warning(f"Validation batch {batch_idx}: Loss is zero. Targets shape: {targets.shape}, Predictions keys: {list(predictions.keys()) if isinstance(predictions, dict) else 'not dict'}")
            
            if batch_idx == 0:
                logger.debug(f"First batch loss: {loss.item():.4f}, loss_breakdown keys: {list(loss_breakdown.keys())}")
                
        except Exception as e:
            logger.error(f"Error computing loss in validation batch {batch_idx}: {e}")
            return {'loss': 0.0}
        
        # Collect predictions and targets for metrics computation
        self._collect_predictions_and_targets(
            predictions, targets, images, device, batch_idx, all_predictions, all_targets, phase_num
        )
        
        return {'loss': loss.item(), 'loss_breakdown': loss_breakdown}
    
    def _collect_predictions_and_targets(self, predictions, targets, images, device, batch_idx,
                                       all_predictions, all_targets, phase_num):
        """Collect predictions and targets for metrics computation."""
        try:
            # Optimization: Only process active layers for current phase
            active_layers = self._get_active_layers_for_phase(phase_num)
            
            # Debug: Log what we're receiving vs what we expect
            if batch_idx == 0:
                logger.info(f"ðŸ” Phase {phase_num} - Received prediction layers: {list(predictions.keys()) if isinstance(predictions, dict) else 'not dict'}")
                logger.info(f"ðŸ” Phase {phase_num} - Expected active layers: {active_layers}")
            
            # Track successful layer processing for debugging static metrics
            processed_layers = 0
            failed_layers = []
            
            # Handle case where predictions might not be a dictionary
            if not isinstance(predictions, dict):
                logger.warning(f"Predictions is not a dict: {type(predictions)}, converting to dict with 'layer_1' key")
                predictions = {'layer_1': predictions}
            
            # Ensure we have at least layer_1 for Phase 1
            if phase_num == 1 and 'layer_1' not in predictions:
                logger.error(f"ðŸš¨ CRITICAL: Phase 1 missing layer_1 predictions! Available: {list(predictions.keys())}")
            
            for layer_name, layer_preds in predictions.items():
                # Skip inactive layers to reduce complexity
                if layer_name not in active_layers:
                    if batch_idx == 0:
                        logger.debug(f"Skipping inactive layer {layer_name} in Phase {phase_num}")
                    continue
                    
                if layer_name not in all_predictions:
                    all_predictions[layer_name] = []
                    all_targets[layer_name] = []
                
                try:
                    # Process for classification metrics
                    layer_output = self.prediction_processor.extract_classification_predictions(
                        layer_preds, images.shape[0], device
                    )
                    
                    # Extract target classes with phase-aware filtering
                    layer_targets = self.prediction_processor.extract_target_classes(
                        targets, images.shape[0], device, layer_name
                    )
                    
                    # Validate outputs before adding to collections
                    if layer_output is not None and layer_targets is not None:
                        if layer_output.numel() > 0 and layer_targets.numel() > 0:
                            all_predictions[layer_name].append(layer_output)
                            all_targets[layer_name].append(layer_targets)
                            processed_layers += 1
                            
                            # Log successful processing
                            if batch_idx <= 1:  # Log first two batches
                                logger.debug(f"âœ… {layer_name} batch {batch_idx}: pred shape {layer_output.shape}, target shape {layer_targets.shape}")
                        else:
                            logger.warning(f"âš ï¸ {layer_name} batch {batch_idx}: Empty tensors - pred numel={layer_output.numel()}, target numel={layer_targets.numel()}")
                            # Try to create dummy tensors to avoid completely missing metrics
                            if layer_output.numel() == 0:
                                layer_output = torch.zeros(1, dtype=torch.long, device=device)
                            if layer_targets.numel() == 0:
                                layer_targets = torch.zeros(1, dtype=torch.long, device=device)
                            all_predictions[layer_name].append(layer_output)
                            all_targets[layer_name].append(layer_targets)
                            processed_layers += 1
                    else:
                        logger.warning(f"âš ï¸ {layer_name} batch {batch_idx}: None outputs - pred={layer_output is not None}, target={layer_targets is not None}")
                        # Create dummy tensors to avoid completely missing metrics
                        if layer_output is None:
                            layer_output = torch.zeros(1, dtype=torch.long, device=device)
                        if layer_targets is None:
                            layer_targets = torch.zeros(1, dtype=torch.long, device=device)
                        all_predictions[layer_name].append(layer_output)
                        all_targets[layer_name].append(layer_targets)
                        processed_layers += 1
                    
                    # Memory-efficient accumulation
                    COMPACT_FREQUENCY = self.config.get('training', {}).get('validation', {}).get('memory_compact_freq', 20)
                    if batch_idx > 0 and batch_idx % COMPACT_FREQUENCY == 0:
                        if len(all_predictions[layer_name]) > 1:
                            # Compact tensors to improve memory locality and reduce fragmentation
                            try:
                                all_predictions[layer_name] = [torch.cat(all_predictions[layer_name], dim=0)]
                                all_targets[layer_name] = [torch.cat(all_targets[layer_name], dim=0)]
                            except Exception as compact_e:
                                logger.warning(f"Warning compacting tensors for {layer_name}: {compact_e}")
                    
                    if batch_idx == 0:
                        logger.debug(f"Layer {layer_name}: prediction shape {layer_output.shape}, target shape {layer_targets.shape}")
                        logger.debug(f"  Prediction sample: {layer_output[:2] if layer_output.numel() > 0 else 'empty'}")
                        logger.debug(f"  Target sample: {layer_targets[:2] if layer_targets.numel() > 0 else 'empty'}")
                        
                except Exception as layer_e:
                    logger.error(f"âŒ Error processing {layer_name} in batch {batch_idx}: {layer_e}")
                    failed_layers.append(f"{layer_name}(exception)")
                    import traceback
                    logger.error(f"   â€¢ Traceback: {traceback.format_exc()}")
            
            # Summary logging for debugging static metrics issue (only log errors)
            if failed_layers:
                logger.error(f"   â€¢ Failed layers: {failed_layers}")
                logger.error(f"   â€¢ This will contribute to missing layer_1_* metrics and static validation values!")
                    
        except Exception as e:
            logger.error(f"âŒ Critical error processing predictions in batch {batch_idx}: {e}")
            logger.error(f"   â€¢ This will cause missing layer_1_* metrics and static validation fallback!")
            import traceback
            logger.error(f"   â€¢ Traceback: {traceback.format_exc()}")
    
    def _get_active_layers_for_phase(self, phase_num: int) -> list:
        """Get list of active layers for the given phase."""
        if phase_num == 1:
            return ['layer_1']  # Phase 1: only layer_1 is active
        elif phase_num == 2:
            return ['layer_1', 'layer_2', 'layer_3']  # Phase 2: all layers are active
        else:
            # Default to all layers for unknown phases or single-phase mode
            return ['layer_1', 'layer_2', 'layer_3']