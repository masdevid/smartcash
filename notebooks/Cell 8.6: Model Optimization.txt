# ================= [MEMORY OPTIMIZATION] =================
# Cell untuk optimasi memori khusus Google Colab

class MemoryOptimizer:
    """Kelas untuk mengoptimalkan penggunaan memori di lingkungan Colab"""
    
    def __init__(self, logger=None):
        self.logger = logger or SmartCashLogger("memory_optimizer")
        
    def check_gpu_status(self):
        """Cek status GPU dan RAM"""
        try:
            # Cek apakah GPU tersedia
            if torch.cuda.is_available():
                # Info GPU
                gpu_name = torch.cuda.get_device_name(0)
                memory_allocated = torch.cuda.memory_allocated(0) / (1024**2)
                memory_reserved = torch.cuda.memory_reserved(0) / (1024**2)
                
                print(f"üñ•Ô∏è GPU: {gpu_name}")
                print(f"üíæ GPU Memory Terpakai: {memory_allocated:.2f} MB")
                print(f"üíæ GPU Memory Dicadangkan: {memory_reserved:.2f} MB")
                
                # Cek persentase penggunaan
                if torch.cuda.is_available():
                    try:
                        # Pendekatan 1: Menggunakan pynvml
                        import pynvml
                        pynvml.nvmlInit()
                        handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                        total_mem = info.total / (1024**2)
                        used_mem = info.used / (1024**2)
                        free_mem = info.free / (1024**2)
                        used_percent = (used_mem / total_mem) * 100
                        
                        print(f"üíæ Total GPU Memory: {total_mem:.2f} MB")
                        print(f"üíæ GPU Memory Bebas: {free_mem:.2f} MB")
                        print(f"üìä Penggunaan GPU: {used_percent:.2f}%")
                    except:
                        # Pendekatan 2: Menggunakan subprocess untuk nvidia-smi
                        try:
                            import subprocess
                            result = subprocess.check_output(
                                ['nvidia-smi', '--query-gpu=memory.total,memory.used,memory.free', '--format=csv,nounits,noheader'],
                                encoding='utf-8'
                            )
                            total_mem, used_mem, free_mem = map(int, result.strip().split(','))
                            used_percent = (used_mem / total_mem) * 100
                            
                            print(f"üíæ Total GPU Memory: {total_mem} MB")
                            print(f"üíæ GPU Memory Bebas: {free_mem} MB")
                            print(f"üìä Penggunaan GPU: {used_percent:.2f}%")
                        except:
                            pass
            else:
                print("‚ùå GPU tidak tersedia, menggunakan CPU")
            
            # Cek RAM
            try:
                import psutil
                ram = psutil.virtual_memory()
                ram_total = ram.total / (1024**3)  # GB
                ram_used = ram.used / (1024**3)    # GB
                ram_free = ram.available / (1024**3)  # GB
                ram_percent = ram.percent
                
                print(f"üñ•Ô∏è Total RAM: {ram_total:.2f} GB")
                print(f"üíæ RAM Terpakai: {ram_used:.2f} GB")
                print(f"üíæ RAM Bebas: {ram_free:.2f} GB")
                print(f"üìä Penggunaan RAM: {ram_percent:.2f}%")
            except:
                pass
                
        except Exception as e:
            self.logger.error(f"‚ùå Gagal memeriksa status GPU: {str(e)}")
    
    def clear_gpu_memory(self):
        """Bersihkan memori GPU"""
        try:
            if torch.cuda.is_available():
                # Catat penggunaan awal
                memory_before = torch.cuda.memory_allocated(0) / (1024**2)
                
                # Bersihkan cache dan memori tidak terpakai
                torch.cuda.empty_cache()
                import gc
                gc.collect()
                
                # Catat penggunaan setelah
                memory_after = torch.cuda.memory_allocated(0) / (1024**2)
                memory_freed = memory_before - memory_after
                
                self.logger.success(f"‚úÖ Memori GPU dibersihkan")
                self.logger.info(f"üìä Memori dibebaskan: {memory_freed:.2f} MB")
                
                return memory_freed
            else:
                self.logger.info("‚ÑπÔ∏è GPU tidak tersedia, tidak perlu membersihkan")
                return 0
        except Exception as e:
            self.logger.error(f"‚ùå Gagal membersihkan memori GPU: {str(e)}")
            return 0
    
    def optimize_for_inference(self, model):
        """Optimasi model untuk inferensi"""
        try:
            # Pastikan mode evaluasi
            model.eval()
            
            # Konversi ke half precision jika GPU tersedia
            if torch.cuda.is_available():
                model = model.half()  # 16-bit floating-point
                self.logger.info("‚úÖ Model dikonversi ke FP16 (half precision)")
            
            # Coba optimasi untuk inferensi dengan torch.jit
            try:
                model = torch.jit.optimize_for_inference(torch.jit.script(model))
                self.logger.info("‚úÖ Model dioptimasi untuk inferensi dengan TorchScript")
            except:
                # Optimasi torchscript gagal, gunakan pendekatan lain
                pass
            
            return model
        except Exception as e:
            self.logger.error(f"‚ùå Gagal mengoptimasi model: {str(e)}")
            return model
    
    def optimize_batch_size(self, model, target_memory_usage=0.7):
        """Cari batch size optimal menggunakan binary search"""
        if not torch.cuda.is_available():
            self.logger.info("‚ÑπÔ∏è GPU tidak tersedia, menggunakan batch size default (8)")
            return 8
        
        try:
            # Cari total memory GPU
            try:
                import pynvml
                pynvml.nvmlInit()
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                total_mem = info.total / (1024**3)  # GB
            except:
                # Estimasi berdasarkan GPU Colab standar (~15GB)
                total_mem = 15.0
            
            # Target memory usage dalam GB
            target_mem = total_mem * target_memory_usage
            
            # Binary search untuk batch size optimal
            min_batch = 1
            max_batch = 128
            optimal_batch = 8  # Default
            
            # Bersihkan memori sebelum mulai
            self.clear_gpu_memory()
            
            while min_batch <= max_batch:
                mid_batch = (min_batch + max_batch) // 2
                
                try:
                    # Buat input dummy
                    dummy_input = torch.randn(mid_batch, 3, 640, 640, device='cuda')
                    
                    # Coba jalankan forward pass
                    with torch.no_grad():
                        model(dummy_input)
                    
                    # Berhasil, coba lebih besar
                    optimal_batch = mid_batch
                    min_batch = mid_batch + 1
                    
                    # Bersihkan setelah satu uji coba
                    del dummy_input
                    torch.cuda.empty_cache()
                except RuntimeError as e:
                    # Out of memory, coba lebih kecil
                    max_batch = mid_batch - 1
                    
                    # Bersihkan setelah error
                    torch.cuda.empty_cache()
                    import gc
                    gc.collect()
            
            self.logger.success(f"‚úÖ Batch size optimal: {optimal_batch}")
            return optimal_batch
            
        except Exception as e:
            self.logger.error(f"‚ùå Gagal menentukan batch size optimal: {str(e)}")
            return 8  # Default batch size

# Inisialisasi memory optimizer
memory_optimizer = MemoryOptimizer(logger)

# UI untuk memori management
check_memory_button = widgets.Button(
    description='Cek Status Memori',
    button_style='info',
    icon='server'
)

clear_memory_button = widgets.Button(
    description='Bersihkan Memori GPU',
    button_style='warning',
    icon='trash'
)

memory_output = widgets.Output()

def on_check_memory_button_clicked(b):
    with memory_output:
        clear_output()
        memory_optimizer.check_gpu_status()

def on_clear_memory_button_clicked(b):
    with memory_output:
        clear_output()
        freed_mem = memory_optimizer.clear_gpu_memory()
        print(f"‚úÖ Berhasil membersihkan {freed_mem:.2f} MB memori GPU")
        
        # Tampilkan status setelah pembersihan
        memory_optimizer.check_gpu_status()

check_memory_button.on_click(on_check_memory_button_clicked)
clear_memory_button.on_click(on_clear_memory_button_clicked)

# Tampilkan UI
display(widgets.VBox([
    widgets.HTML("<h2>üß† Optimasi Memori</h2>"),
    widgets.HTML("<p>Tools untuk mengoptimalkan penggunaan memori di Google Colab.</p>"),
    widgets.HBox([check_memory_button, clear_memory_button]),
    memory_output
]))

# Tampilkan status memori awal
with memory_output:
    memory_optimizer.check_gpu_status()