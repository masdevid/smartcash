# ðŸ’¸ Custom YOLOv5 Architecture for Banknote Detection

This document outlines the architecture and rationale for a YOLOv5-based model using a **pretrained EfficientNet-B4 backbone**, designed to detect multiple object layers in paper currency images.

---

## ðŸ§  Objective
- Build a YOLOv5 model using EfficientNet-B4 backbone with multilayer detection head
- Compare backbone performance between:
  - CSPDarknet (Small:640x640)
  - EfficientNet-B4 (Small:640x640)
- Prediction test using augmented data scenarios:
  - Position preset
  - Lighting preset

## Model Architecture
```json
{
  "backbone": {
    "type": ["EfficientNet-B4", "CSPDarknet"],
    "pretrained": true,
    "source": "data/pretrained" | "timm" | "YOLOv5 Repository"
  },
  "neck": {
    "type": "PAN/FPN",
    "compatible_with": "YOLOv5",
    "pretrained": true
  },
  "heads": [
    {
      "name": "layer_1_head",
      "description": "Detects full note bounding boxes",
      "init": "from_scratch"
    },
    {
      "name": "layer_2_head",
      "description": "Detects denomination-specific visual markers",
      "init": "from_scratch"
    },
    {
      "name": "layer_3_head",
      "description": "Detects common features across all notes",
      "init": "from_scratch"
    }
  ]
}
```

### Pretrained Strategy
```json
{
  "backbone": "pretrained on ImageNet",
  "neck": "pretrained or reused from YOLOv5",
  "detection_heads": "trained from scratch"
}
```

### Class Layers
```json
{
  "layer_1": {
    "description": "Full banknote detection (main object)",
    "examples": ["100K IDR", "50K IDR"],
    "classes": ["001", "002", "005", "010", "020", "050", "100"]
  },
  "layer_2": {
    "description": "Nominal-defining features (unique visual cues)",
    "examples": ["Large printed number", "Nominal Text", "Braile"],
    "classes": ["l2_001", "l2_002", "l2_005", "l2_010", "l2_020", "l2_050", "l2_100"]
  },
  "layer_3": {
    "description": "Common features (shared among notes)",
    "examples": ["BI Logo", "Serial Number & Micro Text", "Security Thread"],
    "classes": ["l3_sign", "l3_text", "l3_thread"]
  }
}
```

### Dataset Format & Handling
```json 
{
  "annotation_format": "YOLOv5PyTorch format",
  "layers": {
    "layer_1": "bounding boxes of entire banknotes",
    "layer_2": "objects that define denomination class",
    "layer_3": "shared/common objects across notes"
  },
  "missing_layer_handling": "mask loss during training when layer data is missing"
}
```

## Training Configuration
```json
{
  "phase_1": "Freeze backbone, train detection heads only",
  "phase_2": "Unfreeze entire model for fine-tuning",
  "loss_function": "total_loss = Î»1 * loss_layer1 + Î»2 * loss_layer2 + Î»3 * loss_layer3",
  "loss_weighting": "uncertainty-based dynamic weighting"
}
```

```python
optimizer = Adam([
    {'params': backbone.parameters(), 'lr': 1e-5},
    {'params': head.parameters(), 'lr': 1e-3}
])
```

### Uncertainty-Based Dynamic Weighting
From the paper "Multi-Task Learning Using Uncertainty to Weigh Losses" by Kendall et al. (Google DeepMind).

#### ðŸ§® Formula (for regression-style or detection losses)
For a task i with loss Láµ¢, and learnable log-variance Ïƒáµ¢Â²:
```python
Láµ¢_weighted = (1 / (2 * Ïƒáµ¢Â²)) * Láµ¢ + log(Ïƒáµ¢)
```
The total loss becomes:
```python
total_loss = Î£ [(1 / (2 * Ïƒáµ¢Â²)) * Láµ¢ + log(Ïƒáµ¢)]
```
Where:
- Ïƒáµ¢ is the task-specific uncertainty parameter (learned during training)
- log(Ïƒáµ¢) acts as a regularizer

#### PyTorch Implementation Snippet
```python
class MultiTaskLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.log_sigma1 = nn.Parameter(torch.tensor(0.0))
        self.log_sigma2 = nn.Parameter(torch.tensor(0.0))
        self.log_sigma3 = nn.Parameter(torch.tensor(0.0))

    def forward(self, loss1, loss2, loss3):
        loss = (
            (1.0 / (2 * torch.exp(self.log_sigma1)**2)) * loss1 + self.log_sigma1 +
            (1.0 / (2 * torch.exp(self.log_sigma2)**2)) * loss2 + self.log_sigma2 +
            (1.0 / (2 * torch.exp(self.log_sigma3)**2)) * loss3 + self.log_sigma3
        )
        return loss
```

## ðŸŽ¯ Training Modes & Prediction Structure
1. **Single-Phase + Multi-Layer** (training_mode='single_phase', layer_mode='multi'):
   - Return 3 predictions: {'layer_1': pred, 'layer_2': pred, 'layer_3': pred}
   - Processes all layers in a single training phase
2. **Single-Phase + Single-Layer** (training_mode='single_phase', layer_mode='single'):
   - Return 1 prediction: {'layer_1': pred}
   - Processes only the primary layer
3. **Two-Phase Mode**:
   - Phase 1: Return 1 prediction: {'layer_1': pred}
   - Phase 2: Return 3 predictions: {'layer_1': pred, 'layer_2': pred, 'layer_3': pred}

## ðŸ”§ Hierarchical Validation System

### Phase-Aware Validation Processing
- **Phase 1 (Frozen Backbone)**: Standard single-layer validation for classes 0-6
- **Phase 2 (Unfrozen Backbone)**: Hierarchical multi-layer validation with confidence modulation

### Layer Architecture & Class Distribution
```
Layer 1: Denomination Detection (Classes 0-6)
â”œâ”€â”€ 001, 002, 005, 010, 020, 050, 100 (Indonesian Rupiah denominations)
â”œâ”€â”€ Purpose: Primary task - banknote denomination identification
â””â”€â”€ Metrics: Primary validation metrics (mAP, precision, recall, F1)

Layer 2: Confidence Features (Classes 7-13)  
â”œâ”€â”€ Denomination-specific visual cues and features
â”œâ”€â”€ Purpose: Enhanced denomination validation through visual features
â””â”€â”€ Integration: Spatial overlap + confidence modulation with Layer 1

Layer 3: Money Validation (Classes 14-16)
â”œâ”€â”€ Security features and authenticity markers
â”œâ”€â”€ Purpose: General money validation (authentic banknote detection)
â””â”€â”€ Integration: Money authenticity threshold + confidence boost/reduction
```

### Hierarchical Processing Flow (Phase 2 Only)
```
Input: All predictions (classes 0-16)
    â†“
Phase Detection: max_class >= 7 â†’ Phase 2 hierarchical processing
    â†“
Layer 1 Filtering: Extract classes 0-6 for primary evaluation
    â†“
Confidence Modulation:
    â€¢ Layer 2 Match: Same denomination + spatial IoU > 0.1
    â€¢ Layer 3 Match: Money validation + spatial IoU > 0.1
    â€¢ Hierarchical Boost: conf Ã— (1 + layer2_conf Ã— layer3_conf) if layer3_conf > 0.1
    â€¢ Confidence Reduction: conf Ã— 0.1 if layer3_conf â‰¤ 0.1 (not money)
    â†“
Metrics Calculation: mAP, precision, recall, F1 on enhanced Layer 1 predictions
```

### Key Benefits
- **Focused Evaluation**: Primary metrics measure denomination detection quality (main task)
- **Intelligent Filtering**: Layer 3 ensures predictions are validated as actual money
- **Research Insights**: Per-layer metrics preserved for detailed analysis
- **Phase Consistency**: Phase 1 establishes baseline, Phase 2 adds hierarchical enhancement
- **Memory Optimization**: Chunked processing for large prediction sets (>10K predictions)

## ðŸ“Š Loss Calculation Strategy

### Phase 1: Frozen Backbone Training
```
Description: Backbone frozen, only train Layer 1 head (coarse/global detection)
Backbone State: frozen
Active Layers: layer_1 only
Loss Weights:
â”œâ”€â”€ layer_1: 1.0 (full weight)
â”œâ”€â”€ layer_2: 0.0 (not trained)
â””â”€â”€ layer_3: 0.0 (not trained)
Purpose: Stabilize initial learning and establish baseline detection
Optional: Small warmup weights (0.1) for layer_2 and layer_3 can be added
```

### Phase 2: Uncertainty-Weighted Multi-Task Learning
```
Description: Backbone unfrozen, fine-tune all layers with uncertainty-based weighting
Backbone State: unfrozen  
Active Layers: layer_1, layer_2, layer_3
Loss Calculation Method: uncertainty_weighted_loss
Mathematical Formulation:
L_total = Î£ (1 / (2 * Ïƒ_iÂ²)) * L_i + log(Ïƒ_i)
Where:
â”œâ”€â”€ L_i: Loss for layer i (i = 1, 2, 3)
â”œâ”€â”€ Ïƒ_i: Learnable uncertainty parameter for layer i
â”œâ”€â”€ 1/(2*Ïƒ_iÂ²): Automatic weight based on uncertainty
â””â”€â”€ log(Ïƒ_i): Regularization term to prevent Ïƒ_i â†’ 0
Layer-Specific Weights:
â”œâ”€â”€ Layer 1: L1_weight = 1 / (2 * Ïƒ1Â²), learnable Ïƒ1
â”œâ”€â”€ Layer 2: L2_weight = 1 / (2 * Ïƒ2Â²), learnable Ïƒ2  
â””â”€â”€ Layer 3: L3_weight = 1 / (2 * Ïƒ3Â²), learnable Ïƒ3
```

### Key Benefits of Uncertainty Weighting
- **Adaptive Balancing**: Automatically adjusts contribution of each layer based on task difficulty
- **Prevents Domination**: No single layer can dominate the loss function
- **Multi-Task Optimization**: Improved multi-task learning through uncertainty-based weighting
- **Self-Regulating**: Learnable Ïƒ parameters adapt during training to optimal values
- **Mathematical Foundation**: Based on principled uncertainty estimation in multi-task learning

## ðŸ”„ Critical Two-Phase Training Weight Transfer Flow

### Phase 1 â†’ Phase 2 Transition Process
This is one of the most critical aspects of the two-phase training system, ensuring Phase 1 learning is preserved when transitioning to Phase 2.

#### Step-by-Step Weight Transfer Flow:
```
1. Phase 1 Training Completion
   â”œâ”€â”€ Model: Frozen backbone + trained detection head
   â”œâ”€â”€ Checkpoint: Save best Phase 1 model (frozen state)
   â””â”€â”€ Architecture: Single task optimized (backbone frozen)
2. Phase 2 Transition Preparation
   â”œâ”€â”€ Load Phase 1 checkpoint data into memory
   â”œâ”€â”€ Extract model configuration from checkpoint
   â”œâ”€â”€ Validate config matches actual model architecture
   â””â”€â”€ Detect/correct any config mismatches via inference
3. Phase 2 Model Rebuilding
   â”œâ”€â”€ Build new model: same architecture + unfrozen backbone
   â”œâ”€â”€ CRITICAL: Set pretrained=False (don't load YOLOv5s weights)
   â”œâ”€â”€ Ensure exact architectural match (classes, layers, backbone)
   â””â”€â”€ Result: Fresh Phase 2 model ready for weight loading
4. Weight Transfer Execution
   â”œâ”€â”€ Load Phase 1 state_dict into Phase 2 model
   â”œâ”€â”€ Use strict=False to handle minor mismatches
   â”œâ”€â”€ Log missing/unexpected keys for debugging
   â””â”€â”€ Fallback: Use fresh weights if transfer fails
5. Phase 2 Training Continuation
   â”œâ”€â”€ Model: Unfrozen backbone + Phase 1 trained weights
   â”œâ”€â”€ Training: Fine-tune entire model (backbone + heads)
   â””â”€â”€ Optimization: All parameters now trainable
```

#### Critical Configuration Validation:
The system performs intelligent validation to ensure weight transfer compatibility:
```python
# Architecture Validation Logic
saved_config = checkpoint['model_config']
actual_output_channels = state_dict['detection_head'].shape[0]
# Example: 66 output channels = 17 classes (multi-layer)
# But saved config shows num_classes=7 â†’ MISMATCH DETECTED
if actual_output_channels == 66 and saved_config['num_classes'] != 17:
    # Trigger intelligent config inference
    inferred_config = infer_from_architecture(checkpoint, checkpoint_path)
    # Use inferred config: backbone='efficientnet_b4', num_classes=17
```

#### Weight Transfer Scenarios & Handling:
**âœ… Successful Transfer:**
```
Phase 1 Model (frozen) â†’ Phase 2 Model (unfrozen)
â”œâ”€â”€ Backbone weights: Transferred successfully  
â”œâ”€â”€ Detection head: Transferred successfully
â”œâ”€â”€ Training state: Preserved and continued
â””â”€â”€ Result: Phase 1 learning preserved in Phase 2
```
**âš ï¸ Architecture Mismatch (Fixed):**
```
Problem: Saved config mismatch (7 vs 17 classes)
â”œâ”€â”€ Detection: Config validation detects mismatch
â”œâ”€â”€ Solution: Intelligent inference from architecture
â”œâ”€â”€ Action: Rebuild with correct configuration
â””â”€â”€ Result: Successful weight transfer with correct config
```
**âŒ Transfer Failure (Fallback):**
```
Critical mismatch preventing weight loading
â”œâ”€â”€ Fallback: Phase 2 starts with fresh initialization
â”œâ”€â”€ Loss: Phase 1 training progress is lost
â”œâ”€â”€ Mitigation: Improved config validation prevents this
â””â”€â”€ Logs: Clear error messages for debugging
```

## ðŸš€ Implementation Status

### ðŸ“ Core Model Architecture
#### Backbone Implementations
- **EfficientNet-B4**: `smartcash/model/architectures/backbones/efficientnet.py`
  - Enhanced with multi-layer detection support
  - Methods: `build_for_yolo()`, `prepare_for_training()`
  - Compatible with YOLOv5 neck architecture
- **CSPDarknet**: `smartcash/model/architectures/backbones/cspdarknet.py`
  - Enhanced with multi-layer detection support
  - Fixed out_channels compatibility with BaseBackbone
  - Methods: `build_for_yolo()`, `prepare_for_training()`
#### Multi-Layer Detection Heads
- **Implementation**: `smartcash/model/architectures/heads/multi_layer_head.py`
  - `MultiLayerHead` class with channel attention mechanisms
  - Factory functions for banknote detection heads
  - Support for 3-layer detection system (layer_1, layer_2, layer_3)
#### Uncertainty-Based Multi-Task Loss
- **Implementation**: `smartcash/model/training/multi_task_loss.py`
  - `UncertaintyMultiTaskLoss` class implementing Kendall et al. methodology
  - Learnable log-variance parameters for dynamic weighting
  - `AdaptiveMultiTaskLoss` for performance-based weighting
#### Complete Model Builder
- **Implementation**: `smartcash/model/core/yolo_model_builder.py`
  - `SmartCashYOLOv5` class integrating backbone, neck, and heads
  - `YOLOModelBuilder` with testing mode support
  - Comprehensive build pipeline with validation
#### Checkpoint Management
- **Implementation**: `smartcash/model/core/checkpoint_manager.py`
  - **Checkpoint Format**: `best_{model_name}_{backbone}_{date:%Y%m%d}.pt`
  - Automatic naming and cleanup
  - Metadata preservation for model info and metrics

### ðŸŽ¯ Training System
#### Training Service
- **Implementation**: `smartcash/model/training/training_service.py`
  - Two-phase training strategy (freeze backbone â†’ fine-tune)
  - Integration with uncertainty-based loss
  - Progress tracking and UI integration
#### Key Implementation Files
- `pipeline_executor.py`: `_transition_to_phase2_and_train()` - Main transition logic
- `pipeline_executor.py`: `_rebuild_model_for_phase2()` - Model rebuilding with config validation
- `pipeline_executor.py`: `_infer_model_config_from_checkpoint()` - Intelligent config inference
- `checkpoint_manager.py`: Enhanced config saving and extraction
#### Recent Critical Fixes (Aug 2024):
1. **Config Validation**: Detect saved config vs actual architecture mismatches
2. **Intelligent Inference**: Extract correct config from checkpoint structure and filename
3. **Pretrained Weight Prevention**: Disable pretrained weights during Phase 2 rebuild
4. **Enhanced Error Handling**: Better logging and fallback strategies

### ðŸ“Š Evaluation System
#### Evaluation Configuration
- **Config File**: `smartcash/configs/evaluation_config.yaml`
- **Scenarios**: Position Variation, Lighting Variation
- **Metrics**: mAP@0.5, mAP@0.75, Precision, Recall, F1-Score, Accuracy, Inference Time
#### Evaluation Service
- **Implementation**: `smartcash/model/evaluation/evaluation_service.py`
- **Scenario Manager**: `smartcash/model/evaluation/scenario_manager.py`
- **Metrics Computation**: `smartcash/model/evaluation/evaluation_metrics.py`
- **Additional Files**: 
  - `yolov5_map_calculator.py`: Hierarchical mAP calculation with confidence modulation
  - `validation_metrics_computer.py`: Hierarchical validation metrics alignment  
  - `hierarchical_processor.py`: Core hierarchical filtering and confidence modulation logic

### ðŸ“ˆ Performance Targets
Based on the implemented architecture:
- **Expected mAP@0.5**: >0.80 for EfficientNet-B4, >0.75 for CSPDarknet
- **Training Phases**: Phase 1 (freeze backbone, LR: 1e-3), Phase 2 (fine-tune, LR: 1e-5)
- **Evaluation Scenarios**: Position variation (5 augmentations), Lighting variation (5 augmentations)

## ðŸ“¦ Project Structure
```
smartcash/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ architectures/
â”‚   â”‚   â”œâ”€â”€ backbones/           # EfficientNet-B4, CSPDarknet implementations
â”‚   â”‚   â””â”€â”€ heads/               # Multi-layer detection heads
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ yolo_model_builder.py   # Complete model building pipeline
â”‚   â”‚   â””â”€â”€ checkpoint_manager.py   # Checkpoint management with proper naming
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ training_service.py     # Two-phase training orchestrator
â”‚   â”‚   â””â”€â”€ multi_task_loss.py      # Uncertainty-based loss implementation
â”‚   â””â”€â”€ evaluation/
â”‚       â”œâ”€â”€ evaluation_service.py   # Scenario-based evaluation
â”‚       â”œâ”€â”€ scenario_manager.py     # Position/Lighting variation tests
â”‚       â”œâ”€â”€ yolov5_map_calculator.py # Hierarchical mAP calculation
â”‚       â”œâ”€â”€ validation_metrics_computer.py # Hierarchical validation metrics
â”‚       â””â”€â”€ hierarchical_processor.py # Confidence modulation logic
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ evaluation_config.yaml     # Evaluation scenarios and metrics
â”‚   â””â”€â”€ training_config.yaml       # Training parameters and phases
â””â”€â”€ tests/
    â””â”€â”€ model/
        â””â”€â”€ test_model_building.py # Comprehensive test suite
```