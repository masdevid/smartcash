# üß† Memory Optimization Guide for SmartCash Training

## üö® Problem Summary
Your Mac system with 18GB MPS memory limit encounters:
- **MPS OOM**: "MPS backend out of memory (MPS allocated: 16.83 GB)"  
- **Process Killed**: System kills training due to memory exhaustion
- **Semaphore Leaks**: 40+ leaked semaphore objects from multiprocessing

## üõ†Ô∏è Three-Tier Solution

### **Tier 1: Enhanced MPS Training** ‚≠ê *Try First*
**File**: `examples/memory_optimized_efficientnet.py`

```bash
# Conservative MPS training
source venv-test/bin/activate
python examples/memory_optimized_efficientnet.py \
    --backbone efficientnet_b4 \
    --batch-size 1 \
    --gradient-accumulation 8 \
    --phase1-epochs 2 \
    --phase2-epochs 2
```

**Features:**
- ‚úÖ Ultra-small batch sizes (1) with gradient accumulation
- ‚úÖ Aggressive memory cleanup after each epoch
- ‚úÖ Emergency MPS cache clearing
- ‚úÖ Beautiful tqdm progress bars with memory monitoring
- ‚úÖ Detailed epoch metrics display
- ‚úÖ Automatic fallback handling

**When to Use:** If you want to utilize MPS acceleration with memory safety

---

### **Tier 2: Ultra-Low Memory CPU** ‚≠ê *MPS Backup*
**File**: `examples/ultra_low_memory_training.py`

```bash
# CPU-only training with memory conservation
python examples/ultra_low_memory_training.py \
    --backbone efficientnet_b4 \
    --epochs 3
```

**Features:**
- ‚úÖ Forces CPU training (avoids MPS entirely)
- ‚úÖ Conservative threading (2 threads) prevents semaphore leaks
- ‚úÖ Single-phase training reduces memory complexity
- ‚úÖ High gradient accumulation (16x) for effective training
- ‚úÖ Minimal progress tracking to save memory

**When to Use:** When MPS consistently runs out of memory

---

### **Tier 3: Minimal Memory Training** ‚≠ê *Last Resort*
**File**: `examples/minimal_memory_training.py`

```bash
# Extreme memory conservation for process kill prevention
python examples/minimal_memory_training.py
```

**Features:**
- ‚úÖ Single-threaded operation (1 thread only)
- ‚úÖ No multiprocessing (prevents semaphore leaks)
- ‚úÖ No progress bars (saves memory overhead)
- ‚úÖ Extreme gradient accumulation (32x)
- ‚úÖ Aggressive garbage collection
- ‚úÖ Process kill prevention

**When to Use:** When even CPU training gets killed by the system

---

## üìä Configuration Comparison

| Feature | Tier 1 (MPS) | Tier 2 (CPU) | Tier 3 (Minimal) |
|---------|--------------|---------------|-------------------|
| **Device** | MPS | CPU | CPU |
| **Batch Size** | 1 | 1 | 1 |
| **Accumulation** | 8x | 16x | 32x |
| **Threads** | 4 | 2 | 1 |
| **Progress Bars** | ‚úÖ Beautiful | ‚úÖ Simple | ‚ùå None |
| **Memory Cleanup** | Aggressive | Ultra | Extreme |
| **Training Speed** | Fast | Slow | Very Slow |
| **Memory Safety** | High | Very High | Maximum |

---

## üéØ Recommended Usage Strategy

### **Step 1: Try Enhanced MPS**
```bash
# Start with conservative MPS settings
python examples/memory_optimized_efficientnet.py \
    --backbone efficientnet_b4 \
    --batch-size 1 \
    --phase1-epochs 1 \
    --phase2-epochs 1
```

**If successful:** Increase epochs gradually
**If MPS OOM:** Proceed to Step 2

### **Step 2: Fallback to CPU**
```bash
# Ultra-low memory CPU training
python examples/ultra_low_memory_training.py \
    --backbone efficientnet_b4 \
    --epochs 3
```

**If successful:** You can train with CPU
**If process killed:** Proceed to Step 3

### **Step 3: Minimal Memory Mode**
```bash
# Extreme memory conservation
python examples/minimal_memory_training.py
```

**If successful:** Basic training completed
**If still fails:** System needs more RAM or cloud training

---

## üîß Advanced Optimization Options

### **EfficientNet-B4 vs CSPDarkNet**
```bash
# Try lighter CSPDarkNet model if EfficientNet is too heavy
python examples/memory_optimized_efficientnet.py \
    --backbone cspdarknet \
    --batch-size 2 \
    --phase1-epochs 3
```

### **Custom Memory Settings**
```bash
# Fine-tune memory parameters
python examples/memory_optimized_efficientnet.py \
    --backbone efficientnet_b4 \
    --batch-size 1 \
    --gradient-accumulation 16 \
    --force-cpu \
    --disable-tqdm
```

### **Diagnostic Mode**
```bash
# Run with minimal epochs to test memory stability
python examples/memory_optimized_efficientnet.py \
    --backbone efficientnet_b4 \
    --phase1-epochs 1 \
    --phase2-epochs 0
```

---

## üö® Troubleshooting

### **"MPS backend out of memory"**
- ‚úÖ Use `--batch-size 1`
- ‚úÖ Try `examples/ultra_low_memory_training.py`
- ‚úÖ Close other applications to free memory

### **"Process Killed" / "Killed: 9"**
- ‚úÖ Use `examples/minimal_memory_training.py`
- ‚úÖ Restart your Mac to free system memory
- ‚úÖ Close all browser tabs and applications

### **"40 leaked semaphore objects"**
- ‚úÖ All scripts now use `dataloader_num_workers=0`
- ‚úÖ Minimal script uses single-threading
- ‚úÖ Fixed in all training scripts

### **Training Too Slow**
- ‚úÖ Use `--backbone cspdarknet` (lighter model)
- ‚úÖ Reduce epochs: `--phase1-epochs 1 --phase2-epochs 1`
- ‚úÖ Consider cloud training (Google Colab, AWS)

---

## üí° Cloud Training Alternative

If local training continues to fail:

```bash
# Upload your project to Google Colab
# Use T4 GPU with 15GB memory - much more stable than MPS
# All scripts work better in Colab environment
```

**Colab Advantages:**
- ‚úÖ 15GB+ GPU memory available
- ‚úÖ No MPS fragmentation issues
- ‚úÖ Faster training than CPU
- ‚úÖ No semaphore leak problems

---

## üìà Success Metrics

### **Tier 1 Success Indicators:**
- MPS training completes without OOM
- Progress bars show memory cleanup messages
- Detailed epoch metrics displayed

### **Tier 2 Success Indicators:**  
- CPU training completes without process kill
- No semaphore leak warnings
- Basic metrics displayed

### **Tier 3 Success Indicators:**
- Process not killed by system
- Training completes (even if minimal)
- No memory-related errors

---

## üéâ Expected Results

With these optimizations, you should be able to:
- ‚úÖ Train EfficientNet-B4 on your Mac (CPU mode)
- ‚úÖ Avoid process kills and memory errors
- ‚úÖ Eliminate semaphore leaking issues
- ‚úÖ Get working model checkpoints
- ‚úÖ Monitor training progress appropriately

The solutions prioritize **memory stability over speed** - ensuring you can complete training even on memory-constrained systems!