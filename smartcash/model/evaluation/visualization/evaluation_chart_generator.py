"""
File: smartcash/model/evaluation/visualization/evaluation_chart_generator.py
Deskripsi: Chart generator untuk evaluation results dengan scenario comparison dan performance analysis
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
import json
from datetime import datetime

from smartcash.common.logger import get_logger

class EvaluationChartGenerator:
    """ðŸ“Š Generator untuk evaluation charts dan visualizations"""
    
    def __init__(self, config: Dict[str, Any] = None, output_dir: str = 'data/evaluation/charts', logger=None):
        self.config = config or {}
        self.logger = logger or get_logger("evaluation.charts")
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Chart styling
        plt.style.use('seaborn-v0_8')
        sns.set_palette("husl")
        
        self.logger.info(f"ðŸ“Š EvaluationChartGenerator initialized | Output: {self.output_dir}")
    
    def generate_scenario_comparison_chart(self, results_data: Dict[str, Any], 
                                         title: str = "Scenario Performance Comparison") -> str:
        """ðŸ“Š Generate scenario comparison chart (position vs lighting variation)"""
        
        try:
            # Extract scenario data
            scenarios = []
            metrics_data = []
            
            for result in results_data.get('results', []):
                scenario_name = result.get('scenario_name', 'unknown')
                metrics = result.get('metrics', {})
                
                scenarios.append(scenario_name.replace('_', ' ').title())
                metrics_data.append({
                    'mAP': metrics.get('map50', 0),
                    'Precision': metrics.get('map50_precision', 0),
                    'Recall': metrics.get('map50_recall', 0),
                    'F1': metrics.get('map50_f1', 0),
                    'Accuracy': metrics.get('accuracy', 0),
                    'FPS': metrics.get('fps', 0)
                })
            
            if not scenarios:
                self.logger.warning("âš ï¸ No scenario data available for chart generation")
                return ""
            
            # Create figure with subplots
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle(title, fontsize=16, fontweight='bold')
            
            # 1. mAP Comparison
            map_values = [m['mAP'] for m in metrics_data]
            bars1 = ax1.bar(scenarios, map_values, color=['#FF6B6B', '#4ECDC4'], alpha=0.8)
            ax1.set_title('mAP@0.5 Performance', fontweight='bold')
            ax1.set_ylabel('mAP Score')
            ax1.set_ylim(0, max(max(map_values) * 1.2, 0.1))
            
            # Add value labels on bars
            for bar, value in zip(bars1, map_values):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
            
            # 2. Precision/Recall/F1 Comparison
            metrics_names = ['Precision', 'Recall', 'F1']
            x = np.arange(len(scenarios))
            width = 0.25
            
            for i, metric in enumerate(metrics_names):
                values = [m[metric] for m in metrics_data]
                offset = (i - 1) * width
                bars = ax2.bar(x + offset, values, width, label=metric, alpha=0.8)
                
                # Add value labels
                for bar, value in zip(bars, values):
                    height = bar.get_height()
                    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                            f'{value:.3f}', ha='center', va='bottom', fontsize=8)
            
            ax2.set_title('Detection Metrics Comparison', fontweight='bold')
            ax2.set_ylabel('Score')
            ax2.set_xlabel('Scenario')
            ax2.set_xticks(x)
            ax2.set_xticklabels(scenarios)
            ax2.legend()
            ax2.set_ylim(0, max([max([m[metric] for m in metrics_data]) for metric in metrics_names]) * 1.3)
            
            # 3. Performance (FPS) Comparison
            fps_values = [m['FPS'] for m in metrics_data]
            bars3 = ax3.bar(scenarios, fps_values, color=['#45B7D1', '#96CEB4'], alpha=0.8)
            ax3.set_title('Inference Performance', fontweight='bold')
            ax3.set_ylabel('FPS (Frames Per Second)')
            ax3.set_ylim(0, max(max(fps_values) * 1.2, 10))
            
            for bar, value in zip(bars3, fps_values):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{value:.1f}', ha='center', va='bottom', fontweight='bold')
            
            # 4. Robustness Analysis (mAP difference)
            if len(scenarios) == 2:  # Position vs Lighting
                position_map = map_values[0] if 'position' in scenarios[0].lower() else map_values[1]
                lighting_map = map_values[1] if 'lighting' in scenarios[1].lower() else map_values[0]
                
                robustness_data = {
                    'Position Impact': abs(position_map - lighting_map),
                    'Baseline (Lighting)': lighting_map,
                    'Degradation': max(0, lighting_map - position_map)
                }
                
                wedges, texts, autotexts = ax4.pie(
                    [robustness_data['Baseline (Lighting)'], robustness_data['Degradation'], 
                     1 - (robustness_data['Baseline (Lighting)'] + robustness_data['Degradation'])],
                    labels=['Robust Performance', 'Position Degradation', 'Remaining'],
                    autopct='%1.1f%%',
                    colors=['#2ECC71', '#E74C3C', '#BDC3C7'],
                    startangle=90
                )
                ax4.set_title('Robustness Analysis', fontweight='bold')
            else:
                ax4.text(0.5, 0.5, 'Robustness analysis\nrequires 2 scenarios', 
                        ha='center', va='center', transform=ax4.transAxes, fontsize=12)
                ax4.set_xlim(0, 1)
                ax4.set_ylim(0, 1)
            
            plt.tight_layout()
            
            # Save chart
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"scenario_comparison_{timestamp}.png"
            save_path = self.output_dir / filename
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            self.logger.info(f"ðŸ“Š Scenario comparison chart saved: {save_path}")
            return str(save_path)
            
        except Exception as e:
            self.logger.error(f"âŒ Error generating scenario comparison chart: {str(e)}")
            return ""
    
    def generate_checkpoint_performance_chart(self, results_data: Dict[str, Any],
                                            title: str = "Checkpoint Performance Analysis") -> str:
        """ðŸ“Š Generate checkpoint performance comparison chart"""
        
        try:
            # Extract checkpoint data
            checkpoints = []
            performance_data = []
            
            for result in results_data.get('results', []):
                checkpoint_info = result.get('checkpoint_info', {})
                metrics = result.get('metrics', {})
                
                checkpoint_name = checkpoint_info.get('name', 'Unknown')
                backbone = checkpoint_info.get('backbone', 'unknown')
                
                checkpoints.append(f"{backbone}\n({checkpoint_name[:15]}...)" if len(checkpoint_name) > 15 
                                 else f"{backbone}\n({checkpoint_name})")
                performance_data.append({
                    'mAP': metrics.get('map50', 0),
                    'Precision': metrics.get('map50_precision', 0),
                    'Recall': metrics.get('map50_recall', 0),
                    'Inference Time': metrics.get('inference_time', 0),
                    'Size (MB)': checkpoint_info.get('size_mb', 0)
                })
            
            if not checkpoints:
                self.logger.warning("âš ï¸ No checkpoint data available for chart generation")
                return ""
            
            # Create figure
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
            fig.suptitle(title, fontsize=16, fontweight='bold')
            
            # 1. mAP Performance Ranking
            map_values = [p['mAP'] for p in performance_data]
            sorted_indices = sorted(range(len(map_values)), key=lambda i: map_values[i], reverse=True)
            sorted_checkpoints = [checkpoints[i] for i in sorted_indices]
            sorted_map_values = [map_values[i] for i in sorted_indices]
            
            colors = plt.cm.RdYlGn([v/max(sorted_map_values) if max(sorted_map_values) > 0 else 0 
                                   for v in sorted_map_values])
            bars1 = ax1.barh(sorted_checkpoints, sorted_map_values, color=colors, alpha=0.8)
            ax1.set_title('mAP@0.5 Performance Ranking', fontweight='bold')
            ax1.set_xlabel('mAP Score')
            
            # Add value labels
            for i, (bar, value) in enumerate(zip(bars1, sorted_map_values)):
                width = bar.get_width()
                ax1.text(width + 0.005, bar.get_y() + bar.get_height()/2.,
                        f'{value:.3f}', ha='left', va='center', fontweight='bold')
            
            # 2. Performance vs Model Size
            sizes = [p['Size (MB)'] for p in performance_data]
            ax2.scatter(sizes, map_values, s=100, alpha=0.7, c=map_values, cmap='RdYlGn')
            ax2.set_xlabel('Model Size (MB)')
            ax2.set_ylabel('mAP Score')
            ax2.set_title('Performance vs Model Size', fontweight='bold')
            
            # Add checkpoint labels
            for i, (x, y) in enumerate(zip(sizes, map_values)):
                ax2.annotate(checkpoints[i].split('\n')[0], (x, y), 
                           xytext=(5, 5), textcoords='offset points', fontsize=8)
            
            # 3. Inference Speed Comparison
            inference_times = [p['Inference Time'] for p in performance_data]
            fps_values = [1/t if t > 0 else 0 for t in inference_times]
            
            bars3 = ax3.bar(range(len(checkpoints)), fps_values, 
                           color=plt.cm.viridis(np.linspace(0, 1, len(checkpoints))), alpha=0.8)
            ax3.set_title('Inference Speed Comparison', fontweight='bold')
            ax3.set_ylabel('FPS (Frames Per Second)')
            ax3.set_xlabel('Checkpoint')
            ax3.set_xticks(range(len(checkpoints)))
            ax3.set_xticklabels([c.split('\n')[0] for c in checkpoints], rotation=45, ha='right')
            
            # Add value labels
            for bar, value in zip(bars3, fps_values):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                        f'{value:.1f}', ha='center', va='bottom', fontweight='bold')
            
            # 4. Multi-metric Radar Chart
            if len(checkpoints) <= 3:  # Radar chart works best with few items
                categories = ['mAP', 'Precision', 'Recall', 'Speed\n(normalized)', 'Efficiency\n(mAP/Size)']
                
                # Normalize data for radar chart
                normalized_data = []
                for i, p in enumerate(performance_data):
                    efficiency = p['mAP'] / max(p['Size (MB)'], 0.1)  # Avoid division by zero
                    speed_norm = fps_values[i] / max(fps_values) if max(fps_values) > 0 else 0
                    
                    normalized_data.append([
                        p['mAP'],
                        p['Precision'], 
                        p['Recall'],
                        speed_norm,
                        efficiency / max([pe['mAP'] / max(pe['Size (MB)'], 0.1) for pe in performance_data])
                    ])
                
                # Create radar chart
                angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
                angles += angles[:1]  # Complete the circle
                
                ax4 = plt.subplot(2, 2, 4, projection='polar')
                
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
                for i, (data, checkpoint) in enumerate(zip(normalized_data, checkpoints)):
                    data += data[:1]  # Complete the circle
                    ax4.plot(angles, data, 'o-', linewidth=2, label=checkpoint.split('\n')[0], 
                            color=colors[i % len(colors)])
                    ax4.fill(angles, data, alpha=0.25, color=colors[i % len(colors)])
                
                ax4.set_xticks(angles[:-1])
                ax4.set_xticklabels(categories)
                ax4.set_ylim(0, 1)
                ax4.set_title('Multi-metric Comparison', fontweight='bold', pad=20)
                ax4.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
            else:
                ax4.text(0.5, 0.5, f'Radar chart limited to\n3 checkpoints\n(Found: {len(checkpoints)})', 
                        ha='center', va='center', transform=ax4.transAxes, fontsize=12)
            
            plt.tight_layout()
            
            # Save chart
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"checkpoint_performance_{timestamp}.png"
            save_path = self.output_dir / filename
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            self.logger.info(f"ðŸ“Š Checkpoint performance chart saved: {save_path}")
            return str(save_path)
            
        except Exception as e:
            self.logger.error(f"âŒ Error generating checkpoint performance chart: {str(e)}")
            return ""
    
    def generate_denomination_analysis_chart(self, results_data: Dict[str, Any],
                                           title: str = "Denomination Classification Analysis") -> str:
        """ðŸ’° Generate denomination classification analysis chart"""
        
        try:
            # Extract denomination analysis data
            scenarios = []
            confusion_matrices = []
            class_performance = []
            
            for result in results_data.get('results', []):
                scenario_name = result.get('scenario_name', 'unknown')
                additional_data = result.get('additional_data', {})
                
                if 'confusion_matrix' in additional_data:
                    scenarios.append(scenario_name.replace('_', ' ').title())
                    confusion_matrices.append(additional_data['confusion_matrix'])
                    
                    # Extract per-class performance if available
                    class_perf = additional_data.get('class_performance', {})
                    class_performance.append(class_perf)
            
            if not confusion_matrices:
                self.logger.warning("âš ï¸ No confusion matrix data available for denomination analysis")
                return ""
            
            # Create figure
            fig = plt.figure(figsize=(18, 12))
            fig.suptitle(title, fontsize=16, fontweight='bold')
            
            # Denomination class names (0-6 + no detection)
            class_names = ['$1', '$2', '$5', '$10', '$20', '$50', '$100', 'No Detection']
            
            # 1. Confusion Matrices (side by side)
            for i, (scenario, cm) in enumerate(zip(scenarios, confusion_matrices[:2])):  # Limit to 2 scenarios
                ax = plt.subplot(2, 3, i + 1)
                
                # Convert to numpy array if needed
                if isinstance(cm, list):
                    cm = np.array(cm)
                
                # Plot heatmap
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                           xticklabels=class_names, yticklabels=class_names, ax=ax)
                ax.set_title(f'{scenario} - Confusion Matrix', fontweight='bold')
                ax.set_xlabel('Predicted')
                ax.set_ylabel('Actual')
            
            # 2. Class-wise Performance Comparison
            if class_performance:
                ax3 = plt.subplot(2, 3, 3)
                
                # Extract precision, recall, f1 for each class across scenarios
                metrics = ['precision', 'recall', 'f1_score']
                x = np.arange(len(class_names[:-1]))  # Exclude 'No Detection'
                width = 0.25
                
                colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
                
                for i, metric in enumerate(metrics):
                    if len(scenarios) > 0 and metric in class_performance[0]:
                        values = class_performance[0][metric][:7]  # First 7 classes
                        offset = (i - 1) * width
                        bars = ax3.bar(x + offset, values, width, label=metric.title(), 
                                     color=colors[i], alpha=0.8)
                        
                        # Add value labels
                        for bar, value in zip(bars, values):
                            if value > 0:
                                height = bar.get_height()
                                ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                                        f'{value:.2f}', ha='center', va='bottom', fontsize=8)
                
                ax3.set_title('Per-Class Performance', fontweight='bold')
                ax3.set_ylabel('Score')
                ax3.set_xlabel('Denomination Class')
                ax3.set_xticks(x)
                ax3.set_xticklabels(class_names[:-1])  # Exclude 'No Detection'
                ax3.legend()
                ax3.set_ylim(0, 1.2)
            
            # 3. Detection Rate Analysis
            ax4 = plt.subplot(2, 3, 4)
            
            detection_rates = []
            missed_rates = []
            
            for cm in confusion_matrices:
                if isinstance(cm, list):
                    cm = np.array(cm)
                
                total_samples = cm.sum()
                detected_samples = cm[:-1, :-1].sum()  # Exclude 'No Detection' row/column
                missed_samples = cm[:-1, -1].sum()  # Samples classified as 'No Detection'
                
                detection_rate = detected_samples / total_samples if total_samples > 0 else 0
                missed_rate = missed_samples / total_samples if total_samples > 0 else 0
                
                detection_rates.append(detection_rate)
                missed_rates.append(missed_rate)
            
            x_pos = np.arange(len(scenarios))
            bars1 = ax4.bar(x_pos - 0.2, detection_rates, 0.4, label='Detected', color='#2ECC71', alpha=0.8)
            bars2 = ax4.bar(x_pos + 0.2, missed_rates, 0.4, label='Missed', color='#E74C3C', alpha=0.8)
            
            ax4.set_title('Detection vs Missed Rate', fontweight='bold')
            ax4.set_ylabel('Rate')
            ax4.set_xlabel('Scenario')
            ax4.set_xticks(x_pos)
            ax4.set_xticklabels(scenarios)
            ax4.legend()
            ax4.set_ylim(0, 1)
            
            # Add value labels
            for bars in [bars1, bars2]:
                for bar in bars:
                    height = bar.get_height()
                    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                            f'{height:.1%}', ha='center', va='bottom', fontweight='bold')
            
            # 4. Denomination Distribution
            ax5 = plt.subplot(2, 3, 5)
            
            # Calculate true distribution from first confusion matrix
            if confusion_matrices:
                cm = confusion_matrices[0]
                if isinstance(cm, list):
                    cm = np.array(cm)
                
                true_counts = cm.sum(axis=1)[:-1]  # Exclude 'No Detection' row
                
                # Create pie chart
                wedges, texts, autotexts = ax5.pie(
                    true_counts, labels=class_names[:-1], autopct='%1.1f%%',
                    colors=plt.cm.Set3(np.linspace(0, 1, len(class_names)-1)),
                    startangle=90
                )
                ax5.set_title('Ground Truth Distribution', fontweight='bold')
            
            # 5. Performance Summary Table
            ax6 = plt.subplot(2, 3, 6)
            ax6.axis('tight')
            ax6.axis('off')
            
            # Create summary table
            summary_data = []
            for i, scenario in enumerate(scenarios):
                if i < len(confusion_matrices):
                    cm = confusion_matrices[i]
                    if isinstance(cm, list):
                        cm = np.array(cm)
                    
                    total = cm.sum()
                    detected = cm[:-1, :-1].sum()
                    accuracy = np.trace(cm[:-1, :-1]) / cm[:-1, :-1].sum() if cm[:-1, :-1].sum() > 0 else 0
                    detection_rate = detected / total if total > 0 else 0
                    
                    summary_data.append([
                        scenario,
                        f"{total:.0f}",
                        f"{detection_rate:.1%}",
                        f"{accuracy:.1%}"
                    ])
            
            table = ax6.table(cellText=summary_data,
                            colLabels=['Scenario', 'Total Samples', 'Detection Rate', 'Accuracy'],
                            cellLoc='center',
                            loc='center')
            table.auto_set_font_size(False)
            table.set_fontsize(10)
            table.scale(1.2, 1.5)
            ax6.set_title('Performance Summary', fontweight='bold', pad=20)
            
            plt.tight_layout()
            
            # Save chart
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"denomination_analysis_{timestamp}.png"
            save_path = self.output_dir / filename
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            self.logger.info(f"ðŸ’° Denomination analysis chart saved: {save_path}")
            return str(save_path)
            
        except Exception as e:
            self.logger.error(f"âŒ Error generating denomination analysis chart: {str(e)}")
            return ""
    
    def generate_evaluation_dashboard(self, results_data: Dict[str, Any],
                                    title: str = "SmartCash Evaluation Dashboard") -> str:
        """ðŸŽ›ï¸ Generate comprehensive evaluation dashboard"""
        
        try:
            # Create comprehensive dashboard
            fig = plt.figure(figsize=(20, 16))
            fig.suptitle(title, fontsize=20, fontweight='bold')
            
            # Extract key metrics
            scenarios = []
            map_scores = []
            fps_scores = []
            detection_rates = []
            
            for result in results_data.get('results', []):
                scenario_name = result.get('scenario_name', 'unknown')
                metrics = result.get('metrics', {})
                
                scenarios.append(scenario_name.replace('_', ' ').title())
                map_scores.append(metrics.get('map50', 0))
                fps_scores.append(metrics.get('fps', 0))
                
                # Calculate detection rate from additional data
                additional_data = result.get('additional_data', {})
                if 'confusion_matrix' in additional_data:
                    cm = additional_data['confusion_matrix']
                    if isinstance(cm, list):
                        cm = np.array(cm)
                    total = cm.sum()
                    detected = cm[:-1, :-1].sum()
                    detection_rates.append(detected / total if total > 0 else 0)
                else:
                    detection_rates.append(0)
            
            if not scenarios:
                self.logger.warning("âš ï¸ No data available for dashboard generation")
                return ""
            
            # 1. Executive Summary (Top row)
            ax1 = plt.subplot(3, 4, (1, 2))
            
            # Key metrics display
            avg_map = np.mean(map_scores) if map_scores else 0
            avg_fps = np.mean(fps_scores) if fps_scores else 0
            avg_detection = np.mean(detection_rates) if detection_rates else 0
            
            metrics_text = f"""
SMARTCASH MODEL EVALUATION SUMMARY
{'='*50}

ðŸŽ¯ Average mAP@0.5: {avg_map:.3f} ({avg_map*100:.1f}%)
âš¡ Average FPS: {avg_fps:.1f} frames/sec
ðŸ” Average Detection Rate: {avg_detection:.1%}

ðŸ“Š Scenarios Evaluated: {len(scenarios)}
ðŸ† Best Performing: {scenarios[np.argmax(map_scores)] if map_scores else 'N/A'}
âš ï¸ Most Challenging: {scenarios[np.argmin(map_scores)] if map_scores else 'N/A'}

ðŸ’¡ Robustness: {abs(max(map_scores) - min(map_scores)):.3f} mAP spread
            """
            
            ax1.text(0.05, 0.95, metrics_text, transform=ax1.transAxes, fontsize=12,
                    verticalalignment='top', fontfamily='monospace')
            ax1.set_xlim(0, 1)
            ax1.set_ylim(0, 1)
            ax1.axis('off')
            
            # 2. Performance Trend
            ax2 = plt.subplot(3, 4, (3, 4))
            x = np.arange(len(scenarios))
            
            # Create dual y-axis plot
            ax2_twin = ax2.twinx()
            
            line1 = ax2.plot(x, map_scores, 'o-', color='#FF6B6B', linewidth=3, markersize=8, label='mAP@0.5')
            line2 = ax2_twin.plot(x, fps_scores, 's-', color='#4ECDC4', linewidth=3, markersize=8, label='FPS')
            
            ax2.set_xlabel('Scenario')
            ax2.set_ylabel('mAP Score', color='#FF6B6B')
            ax2_twin.set_ylabel('FPS', color='#4ECDC4')
            ax2.set_title('Performance Across Scenarios', fontweight='bold')
            ax2.set_xticks(x)
            ax2.set_xticklabels(scenarios, rotation=45, ha='right')
            
            # Add value labels
            for i, (map_val, fps_val) in enumerate(zip(map_scores, fps_scores)):
                ax2.annotate(f'{map_val:.3f}', (i, map_val), textcoords="offset points", 
                           xytext=(0,10), ha='center', color='#FF6B6B', fontweight='bold')
                ax2_twin.annotate(f'{fps_val:.1f}', (i, fps_val), textcoords="offset points", 
                                xytext=(0,-15), ha='center', color='#4ECDC4', fontweight='bold')
            
            # 3. Detection Quality Heatmap
            ax3 = plt.subplot(3, 4, (5, 6))
            
            # Create quality matrix
            quality_metrics = []
            for result in results_data.get('results', []):
                metrics = result.get('metrics', {})
                quality_metrics.append([
                    metrics.get('map50', 0),
                    metrics.get('map50_precision', 0),
                    metrics.get('map50_recall', 0),
                    metrics.get('map50_f1', 0)
                ])
            
            if quality_metrics:
                quality_matrix = np.array(quality_metrics).T
                metric_names = ['mAP@0.5', 'Precision', 'Recall', 'F1']
                
                sns.heatmap(quality_matrix, annot=True, fmt='.3f', cmap='RdYlGn',
                           xticklabels=scenarios, yticklabels=metric_names, ax=ax3)
                ax3.set_title('Detection Quality Heatmap', fontweight='bold')
            
            # 4. Resource Efficiency
            ax4 = plt.subplot(3, 4, (7, 8))
            
            # Efficiency scatter plot (mAP vs FPS)
            scatter = ax4.scatter(fps_scores, map_scores, s=200, alpha=0.7, 
                                c=range(len(scenarios)), cmap='viridis')
            
            # Add scenario labels
            for i, (fps, map_val, scenario) in enumerate(zip(fps_scores, map_scores, scenarios)):
                ax4.annotate(scenario, (fps, map_val), xytext=(5, 5), 
                           textcoords='offset points', fontsize=10)
            
            ax4.set_xlabel('FPS (Performance)')
            ax4.set_ylabel('mAP@0.5 (Accuracy)')
            ax4.set_title('Accuracy vs Performance Trade-off', fontweight='bold')
            ax4.grid(True, alpha=0.3)
            
            # 5. Robustness Analysis
            ax5 = plt.subplot(3, 4, 9)
            
            if len(map_scores) >= 2:
                robustness_score = 1 - (abs(max(map_scores) - min(map_scores)) / max(map_scores)) if max(map_scores) > 0 else 0
                
                # Gauge chart for robustness
                theta = np.linspace(0, np.pi, 100)
                r = np.ones_like(theta)
                
                # Color sections
                colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']
                sections = np.linspace(0, np.pi, 6)
                
                for i in range(5):
                    mask = (theta >= sections[i]) & (theta < sections[i+1])
                    ax5.fill_between(theta[mask], 0, r[mask], color=colors[i], alpha=0.3)
                
                # Robustness pointer
                pointer_angle = robustness_score * np.pi
                ax5.plot([pointer_angle, pointer_angle], [0, 1], 'k-', linewidth=4)
                ax5.plot(pointer_angle, 1, 'ko', markersize=10)
                
                ax5.set_ylim(0, 1.2)
                ax5.set_xlim(0, np.pi)
                ax5.set_title(f'Robustness Score\n{robustness_score:.2f}', fontweight='bold')
                ax5.set_yticks([])
                ax5.set_xticks([0, np.pi/2, np.pi])
                ax5.set_xticklabels(['Poor', 'Good', 'Excellent'])
                ax5.grid(True, alpha=0.3)
            
            # 6. Detection Rate Distribution
            ax6 = plt.subplot(3, 4, 10)
            
            bars = ax6.bar(scenarios, detection_rates, color=plt.cm.RdYlGn([r for r in detection_rates]), alpha=0.8)
            ax6.set_title('Detection Rate by Scenario', fontweight='bold')
            ax6.set_ylabel('Detection Rate')
            ax6.set_xticklabels(scenarios, rotation=45, ha='right')
            
            # Add percentage labels
            for bar, rate in zip(bars, detection_rates):
                height = bar.get_height()
                ax6.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                        f'{rate:.1%}', ha='center', va='bottom', fontweight='bold')
            
            # 7. Model Comparison (if multiple checkpoints)
            ax7 = plt.subplot(3, 4, 11)
            
            checkpoint_names = []
            for result in results_data.get('results', []):
                checkpoint_info = result.get('checkpoint_info', {})
                checkpoint_names.append(checkpoint_info.get('backbone', 'Unknown'))
            
            unique_checkpoints = list(set(checkpoint_names))
            
            if len(unique_checkpoints) > 1:
                # Compare performance by checkpoint
                checkpoint_performance = {}
                for checkpoint in unique_checkpoints:
                    checkpoint_maps = [map_scores[i] for i, name in enumerate(checkpoint_names) if name == checkpoint]
                    checkpoint_performance[checkpoint] = np.mean(checkpoint_maps) if checkpoint_maps else 0
                
                bars = ax7.bar(checkpoint_performance.keys(), checkpoint_performance.values(), 
                              color=plt.cm.viridis(np.linspace(0, 1, len(unique_checkpoints))), alpha=0.8)
                ax7.set_title('Model Comparison', fontweight='bold')
                ax7.set_ylabel('Average mAP@0.5')
                
                for bar, value in zip(bars, checkpoint_performance.values()):
                    height = bar.get_height()
                    ax7.text(bar.get_x() + bar.get_width()/2., height + 0.005,
                            f'{value:.3f}', ha='center', va='bottom', fontweight='bold')
            else:
                ax7.text(0.5, 0.5, f'Single Model\n{unique_checkpoints[0] if unique_checkpoints else "Unknown"}', 
                        ha='center', va='center', transform=ax7.transAxes, fontsize=14, fontweight='bold')
            
            # 8. System Information
            ax8 = plt.subplot(3, 4, 12)
            
            # System info text
            eval_info = results_data.get('evaluation_info', {})
            timestamp = eval_info.get('timestamp', datetime.now().isoformat())
            
            info_text = f"""
EVALUATION INFO
{'='*20}

ðŸ“… Date: {timestamp[:10]}
â° Time: {timestamp[11:19]}
ðŸ–¥ï¸ System: SmartCash v2.0
ðŸŽ¯ Scenarios: {len(scenarios)}
ðŸ“Š Total Metrics: {len(map_scores) * 4}

ðŸ”§ Configuration:
  â€¢ Detection Layers: Multi
  â€¢ Training Mode: Two-phase
  â€¢ Backbone: Various
  
ðŸ“ˆ Data Quality: âœ… Validated
ðŸ”’ Reproducible: âœ… Logged
            """
            
            ax8.text(0.05, 0.95, info_text, transform=ax8.transAxes, fontsize=10,
                    verticalalignment='top', fontfamily='monospace')
            ax8.set_xlim(0, 1)
            ax8.set_ylim(0, 1)
            ax8.axis('off')
            
            plt.tight_layout()
            
            # Save dashboard
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"evaluation_dashboard_{timestamp}.png"
            save_path = self.output_dir / filename
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            self.logger.info(f"ðŸŽ›ï¸ Evaluation dashboard saved: {save_path}")
            return str(save_path)
            
        except Exception as e:
            self.logger.error(f"âŒ Error generating evaluation dashboard: {str(e)}")
            return ""
    
    def generate_all_charts(self, results_data: Dict[str, Any]) -> List[str]:
        """ðŸ“Š Generate all evaluation charts"""
        
        chart_paths = []
        
        self.logger.info("ðŸŽ¨ Starting comprehensive chart generation...")
        
        # 1. Scenario Comparison
        scenario_chart = self.generate_scenario_comparison_chart(results_data)
        if scenario_chart:
            chart_paths.append(scenario_chart)
        
        # 2. Checkpoint Performance
        checkpoint_chart = self.generate_checkpoint_performance_chart(results_data)
        if checkpoint_chart:
            chart_paths.append(checkpoint_chart)
        
        # 3. Denomination Analysis
        denomination_chart = self.generate_denomination_analysis_chart(results_data)
        if denomination_chart:
            chart_paths.append(denomination_chart)
        
        # 4. Evaluation Dashboard
        dashboard_chart = self.generate_evaluation_dashboard(results_data)
        if dashboard_chart:
            chart_paths.append(dashboard_chart)
        
        self.logger.info(f"âœ… Generated {len(chart_paths)} evaluation charts in {self.output_dir}")
        
        return chart_paths
    
    def get_chart_summary(self) -> Dict[str, Any]:
        """ðŸ“‹ Get summary of generated charts"""
        
        chart_files = list(self.output_dir.glob('*.png'))
        
        return {
            'total_charts': len(chart_files),
            'output_directory': str(self.output_dir),
            'latest_charts': [f.name for f in sorted(chart_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5]],
            'chart_types': {
                'scenario_comparison': len([f for f in chart_files if 'scenario_comparison' in f.name]),
                'checkpoint_performance': len([f for f in chart_files if 'checkpoint_performance' in f.name]),
                'denomination_analysis': len([f for f in chart_files if 'denomination_analysis' in f.name]),
                'evaluation_dashboard': len([f for f in chart_files if 'evaluation_dashboard' in f.name])
            }
        }