class ModelManager:
    """Pengelola model SmartCash dengan dukungan backbone berbeda dan fleksibilitas layer"""
    
    def __init__(self, config, logger=None):
        self.config = config
        self.logger = logger or SmartCashLogger("model_manager")
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Setup layer deteksi yang aktif
        self.detection_layers = config.get('layers', ['banknote'])
        
        # Hitung jumlah kelas per layer
        self.classes_per_layer = {
            'banknote': 7,    # 7 denominasi dasar
            'nominal': 7,     # 7 area nominal 
            'security': 3     # 3 fitur keamanan
        }
        
        # Hitung total kelas berdasarkan layer yang aktif
        self.num_classes = sum(self.classes_per_layer.get(layer, 0) 
                               for layer in self.detection_layers)
        
        # Setup direktori untuk checkpoints
        self.checkpoint_dir = Path(config.get('output_dir', 'runs/train')) / 'weights'
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger.info(f"üîÑ Inisialisasi model dengan {len(self.detection_layers)} layer aktif dan {self.num_classes} kelas")
        
    def create_model(self, backbone_type=None):
        """Buat model YOLOv5 dengan backbone tertentu dan layer yang fleksibel"""
        backbone = backbone_type or self.config.get('model', {}).get('backbone', 'efficientnet')
        pretrained = self.config.get('model', {}).get('pretrained', True)
        
        self.logger.info(f"üèóÔ∏è Membuat model YOLOv5 dengan backbone {backbone} dan layers {self.detection_layers}")
        
        try:
            model = YOLOv5Model(
                num_classes=self.num_classes,
                backbone_type=backbone,
                pretrained=pretrained,
                detection_layers=self.detection_layers
            )
            
            # Pindahkan model ke device yang tersedia
            model = model.to(self.device)
            
            # Log jumlah parameter
            total_params = sum(p.numel() for p in model.parameters())
            trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
            
            self.logger.info(f"üìä Total parameter: {total_params:,}")
            self.logger.info(f"üìä Parameter yang dapat dilatih: {trainable_params:,}")
            
            return model
            
        except Exception as e:
            self.logger.error(f"‚ùå Gagal membuat model: {str(e)}")
            self.logger.error(f"üìã Detail: {str(e)}")
            raise

    def get_optimizer(self, model, lr=None):
        """Dapatkan optimizer untuk model"""
        learning_rate = lr or self.config.get('training', {}).get('learning_rate', 0.001)
        weight_decay = self.config.get('training', {}).get('weight_decay', 0.0005)
        
        self.logger.info(f"üîß Membuat optimizer dengan learning rate {learning_rate}")
        
        # Pilih optimizer berdasarkan konfigurasi
        optimizer_type = self.config.get('training', {}).get('optimizer', 'adam').lower()
        
        if optimizer_type == 'adam':
            return torch.optim.Adam(
                model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif optimizer_type == 'adamw':
            return torch.optim.AdamW(
                model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
        elif optimizer_type == 'sgd':
            momentum = self.config.get('training', {}).get('momentum', 0.9)
            return torch.optim.SGD(
                model.parameters(),
                lr=learning_rate,
                momentum=momentum,
                weight_decay=weight_decay
            )
        else:
            self.logger.warning(f"‚ö†Ô∏è Tipe optimizer '{optimizer_type}' tidak dikenal, menggunakan Adam")
            return torch.optim.Adam(
                model.parameters(),
                lr=learning_rate,
                weight_decay=weight_decay
            )
    
    def get_scheduler(self, optimizer):
        """Dapatkan learning rate scheduler berdasarkan konfigurasi"""
        scheduler_type = self.config.get('training', {}).get('scheduler', 'plateau').lower()
        
        if scheduler_type == 'plateau':
            return torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=0.5,
                patience=5,
                verbose=True
            )
        elif scheduler_type == 'step':
            step_size = self.config.get('training', {}).get('lr_step_size', 10)
            gamma = self.config.get('training', {}).get('lr_gamma', 0.1)
            return torch.optim.lr_scheduler.StepLR(
                optimizer,
                step_size=step_size,
                gamma=gamma
            )
        elif scheduler_type == 'cosine':
            epochs = self.config.get('training', {}).get('epochs', 30)
            return torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer,
                T_max=epochs
            )
        else:
            self.logger.warning(f"‚ö†Ô∏è Tipe scheduler '{scheduler_type}' tidak dikenal, menggunakan ReduceLROnPlateau")
            return torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=0.5,
                patience=5,
                verbose=True
            )
    
    def save_checkpoint(self, model, epoch, loss, is_best=False):
        """Simpan checkpoint model"""
        checkpoint_dir = self.checkpoint_dir
        
        try:
            # Pastikan direktori ada
            os.makedirs(checkpoint_dir, exist_ok=True)
            
            # Simpan model menggunakan StatelessCheckpointSaver
            return StatelessCheckpointSaver.save_checkpoint(
                model=model,
                config=self.config,
                epoch=epoch,
                loss=loss,
                checkpoint_dir=str(checkpoint_dir),
                is_best=is_best,
                log_fn=self.logger.info
            )
        except Exception as e:
            self.logger.error(f"‚ùå Gagal menyimpan checkpoint: {str(e)}")
            self.logger.error(f"üìã Detail: {str(e)}")
            return None
    
    def load_model(self, checkpoint_path=None):
        """Muat model dari checkpoint dengan dukungan fleksibilitas layer"""
        # Jika tidak ada path yang diberikan, cari checkpoint terbaik
        if checkpoint_path is None:
            best_checkpoints = list(self.checkpoint_dir.glob("*_best.pth"))
            if best_checkpoints:
                checkpoint_path = str(max(best_checkpoints, key=os.path.getmtime))
                self.logger.info(f"üìÇ Menggunakan checkpoint terbaik: {checkpoint_path}")
            else:
                latest_checkpoints = list(self.checkpoint_dir.glob("*_latest.pth"))
                if latest_checkpoints:
                    checkpoint_path = str(max(latest_checkpoints, key=os.path.getmtime))
                    self.logger.info(f"üìÇ Menggunakan checkpoint terakhir: {checkpoint_path}")
                else:
                    self.logger.warning("‚ö†Ô∏è Tidak ada checkpoint yang ditemukan")
                    return self.create_model()
                  
        try:
            # Muat checkpoint terlebih dahulu untuk mendapatkan informasi konfigurasi
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            checkpoint_config = checkpoint.get('config', {})
            
            # Dapatkan informasi backbone dan layer dari checkpoint
            backbone_type = checkpoint_config.get('model', {}).get('backbone', self.config.get('model', {}).get('backbone', 'efficientnet'))
            checkpoint_layers = checkpoint_config.get('layers', self.detection_layers)
            
            # Perbarui detection_layers pada instance jika diperlukan
            if set(checkpoint_layers) != set(self.detection_layers):
                self.logger.warning(f"‚ö†Ô∏è Layer pada checkpoint ({checkpoint_layers}) berbeda dengan konfigurasi saat ini ({self.detection_layers})")
                self.logger.info(f"‚ÑπÔ∏è Menggunakan layer dari checkpoint: {checkpoint_layers}")
                self.detection_layers = checkpoint_layers
                
                # Perbarui num_classes
                self.num_classes = sum(self.classes_per_layer.get(layer, 0) 
                                      for layer in self.detection_layers)
            
            # Buat model baru dengan konfigurasi yang sama dengan checkpoint
            model = YOLOv5Model(
                num_classes=self.num_classes,
                backbone_type=backbone_type,
                pretrained=False,  # Tidak perlu pretrained karena akan di-overwrite
                detection_layers=self.detection_layers
            )
            
            # Pindahkan model ke device
            model = model.to(self.device)
            
            # Muat state_dict
            model.load_state_dict(checkpoint['model_state_dict'])
            
            self.logger.success(f"‚úÖ Model berhasil dimuat dari {checkpoint_path}")
            self.logger.info(f"üìä Epoch: {checkpoint.get('epoch', 'unknown')}")
            self.logger.info(f"üìâ Loss: {checkpoint.get('loss', 'unknown')}")
            self.logger.info(f"üîç Backbone: {backbone_type}")
            self.logger.info(f"üìã Layers: {self.detection_layers}")
            
            return model
        except Exception as e:
            self.logger.error(f"‚ùå Gagal memuat model: {str(e)}")
            self.logger.error(f"üìã Detail: {str(e)}")
            
            # Fallback ke model baru
            self.logger.warning("‚ö†Ô∏è Kembali ke model baru dengan konfigurasi default")
            return self.create_model()