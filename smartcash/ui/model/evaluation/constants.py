"""
Constants for evaluation module with 2√ó4 model evaluation matrix.
Total tests: 2 scenarios √ó 2 backbones √ó 2 layer modes = 8 evaluations
"""

from enum import Enum
from typing import Dict, Any, List

class EvaluationOperation(Enum):
    """Evaluation operations."""
    POSITION_VARIATION = "position_variation"
    LIGHTING_VARIATION = "lighting_variation"
    ALL_SCENARIOS = "all_scenarios"
    LOAD_CHECKPOINT = "load_checkpoint"
    GENERATE_REPORT = "generate_report"

class EvaluationPhase(Enum):
    """Evaluation phases."""
    IDLE = "idle"
    LOADING_CHECKPOINT = "loading_checkpoint"
    RUNNING_SCENARIO = "running_scenario"
    GENERATING_REPORT = "generating_report"
    COMPLETED = "completed"
    ERROR = "error"

class TestScenario(Enum):
    """2 Research test scenarios from evaluation_config.yaml."""
    POSITION_VARIATION = "position_variation"
    LIGHTING_VARIATION = "lighting_variation"

class BackboneType(Enum):
    """2 Backbone model types."""
    CSPDARKNET = "cspdarknet"
    EFFICIENTNET_B4 = "efficientnet_b4"

class LayerMode(Enum):
    """2 Layer training modes.""" 
    SINGLE = "single"
    MULTILAYER = "multilayer"

# UI Configuration
UI_CONFIG = {
    'module_name': 'evaluation',
    'parent_module': 'model',
    'title': 'Model Evaluation',
    'subtitle': 'Evaluate model performance across different scenarios and configurations',
    'description': 'Evaluate model performance across 4 research scenarios',
    'icon': 'üéØ',
    'version': '2.0.0'
}

# 2 Research Scenarios from evaluation_config.yaml
RESEARCH_SCENARIOS = {
    "position_variation": {
        "name": "Position Variation",
        "description": "Test model robustness against position and rotation changes",
        "icon": "üìê",
        "color": "#007bff",
        "enabled": True,
        "augmentation_config": {
            "num_variations": 5
        }
    },
    "lighting_variation": {
        "name": "Lighting Variation", 
        "description": "Test model robustness against lighting condition changes",
        "icon": "üí°",
        "color": "#ffc107",
        "enabled": True,
        "augmentation_config": {
            "num_variations": 5
        }
    }
}

# 4 Model combinations: 2 backbones √ó 2 layer modes
MODEL_COMBINATIONS = [
    {"backbone": "cspdarknet", "layer_mode": "single", "name": "CSPDarknet Single-Layer"},
    {"backbone": "cspdarknet", "layer_mode": "multilayer", "name": "CSPDarknet Multi-Layer"},
    {"backbone": "efficientnet_b4", "layer_mode": "single", "name": "EfficientNet-B4 Single-Layer"},
    {"backbone": "efficientnet_b4", "layer_mode": "multilayer", "name": "EfficientNet-B4 Multi-Layer"}
]

# Total evaluation matrix: 2 scenarios √ó 4 models = 8 tests
EVALUATION_MATRIX = []
for scenario in RESEARCH_SCENARIOS.keys():
    for model in MODEL_COMBINATIONS:
        EVALUATION_MATRIX.append({
            "scenario": scenario,
            "backbone": model["backbone"], 
            "layer_mode": model["layer_mode"],
            "test_name": f"{RESEARCH_SCENARIOS[scenario]['name']} - {model['name']}"
        })

# Evaluation metrics configuration
EVALUATION_METRICS = {
    "map": {
        "name": "mAP@0.5", 
        "description": "Mean Average Precision at IoU=0.5",
        "format": "{:.3f}",
        "icon": "üéØ",
        "color": "#007bff"
    },
    "map_75": {
        "name": "mAP@0.75",
        "description": "Mean Average Precision at IoU=0.75", 
        "format": "{:.3f}",
        "icon": "üéØ",
        "color": "#0056b3"
    },
    "precision": {
        "name": "Precision",
        "description": "Detection Precision",
        "format": "{:.3f}",
        "icon": "üîç",
        "color": "#28a745"
    },
    "recall": {
        "name": "Recall",
        "description": "Detection Recall", 
        "format": "{:.3f}",
        "icon": "üìä",
        "color": "#ffc107"
    },
    "f1_score": {
        "name": "F1 Score",
        "description": "F1 Score (2*precision*recall)/(precision+recall)",
        "format": "{:.3f}", 
        "icon": "‚öñÔ∏è",
        "color": "#6f42c1"
    },
    "accuracy": {
        "name": "Accuracy",
        "description": "Overall classification accuracy",
        "format": "{:.3f}",
        "icon": "üéØ",
        "color": "#e83e8c"
    },
    "inference_time": {
        "name": "Inference Time",
        "description": "Average inference time per image",
        "format": "{:.2f}ms",
        "icon": "‚è±Ô∏è", 
        "color": "#20c997"
    }
}

# Button configurations
BUTTON_CONFIG = {
    'run_all_scenarios': {
        'text': 'üöÄ Run All Scenarios',
        'style': 'success',
        'tooltip': 'Run evaluation across all scenarios and models (8 total tests)'
    },
    'run_position_scenario': {
        'text': 'üìê Position Variation',
        'style': 'info',
        'tooltip': 'Run position variation scenario only (4 model tests)'
    },
    'run_lighting_scenario': {
        'text': 'üí° Lighting Variation', 
        'style': 'info',
        'tooltip': 'Run lighting variation scenario only (4 model tests)'
    },
    'load_checkpoint': {
        'text': 'üìÇ Load Models',
        'style': 'warning',
        'tooltip': 'Load best trained model checkpoints for evaluation'
    },
    'export_results': {
        'text': 'üìä Export Results',
        'style': 'secondary',
        'tooltip': 'Export evaluation results and metrics'
    },
    'clear_results': {
        'text': 'üóëÔ∏è Clear Results',
        'style': 'danger',
        'tooltip': 'Clear previous evaluation results'
    }
}

# Operation messages for logging
OPERATION_MESSAGES = {
    EvaluationOperation.POSITION_VARIATION: {
        "start": "üìê Starting position variation scenario...",
        "progress": "üìê Testing position variations... {progress}%",
        "success": "‚úÖ Position variation scenario completed",
        "error": "‚ùå Position variation scenario failed"
    },
    EvaluationOperation.LIGHTING_VARIATION: {
        "start": "üí° Starting lighting variation scenario...",
        "progress": "üí° Testing lighting variations... {progress}%", 
        "success": "‚úÖ Lighting variation scenario completed",
        "error": "‚ùå Lighting variation scenario failed"
    },
    EvaluationOperation.ALL_SCENARIOS: {
        "start": "üöÄ Starting comprehensive evaluation...",
        "progress": "üîÑ Testing {current}/{total} model combinations...",
        "success": "üéâ All scenarios evaluation completed successfully",
        "error": "‚ùå Comprehensive evaluation failed"
    },
    EvaluationOperation.LOAD_CHECKPOINT: {
        "start": "üìÇ Loading model checkpoint...",
        "progress": "‚è≥ Loading model from {checkpoint}...",
        "success": "‚úÖ Model checkpoint loaded successfully",
        "error": "‚ùå Failed to load model checkpoint"
    },
    EvaluationOperation.GENERATE_REPORT: {
        "start": "üìÑ Generating evaluation report...",
        "progress": "üìä Compiling results from {scenarios} scenarios...",
        "success": "üìã Evaluation report generated successfully", 
        "error": "‚ùå Report generation failed"
    }
}

# Default configuration
DEFAULT_EVALUATION_CONFIG = {
    "evaluation": {
        "scenarios": RESEARCH_SCENARIOS,
        "metrics": list(EVALUATION_METRICS.keys()),
        "checkpoint": {
            "path": "data/checkpoints",
            "auto_select_best": True,
            "sort_by": "val_map"
        },
        "output": {
            "results_dir": "data/evaluation/results",
            "export_format": ["json", "csv"],
            "save_images": True
        },
        "inference": {
            "confidence_threshold": 0.25,
            "iou_threshold": 0.45,
            "max_detections": 100,
            "img_size": 640
        }
    }
}

# Available scenarios for UI selection
AVAILABLE_SCENARIOS = list(RESEARCH_SCENARIOS.keys())

# Available metrics for UI selection  
AVAILABLE_METRICS = list(EVALUATION_METRICS.keys())

# Module metadata
MODULE_METADATA = {
    'name': 'evaluation',
    'display_name': 'Model Evaluation',
    'description': 'Comprehensive model evaluation with 4 research scenarios',
    'version': '2.0.0',
    'author': 'SmartCash Team',
    'scenarios_count': len(RESEARCH_SCENARIOS),
    'metrics_count': len(EVALUATION_METRICS)
}